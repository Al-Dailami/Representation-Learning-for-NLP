{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "LEFTOVERS:   \n",
    "\n",
    "\n",
    "    1) Tensor board graphs for loss    \n",
    "    2) Tensor board for network vis   \n",
    "    3) Is embedding good or not ?\n",
    "    4) Embedding visualization ?\n",
    "    5) model perplexity\n",
    "    \n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='Untitled3_1_LOG.txt', mode='a')\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "Harry Potter and the Prisoner of Azkaban\r\n",
      "\r\n",
      "by J.K. Rowling\r\n",
      "\r\n",
      "CHAPTER ONE\r\n",
      "\r\n",
      "OWL POST\r\n",
      "\r\n",
      "Harry Potter was a highly unusual boy in many ways. For one thing, he\r\n",
      "hated the summer holidays more than any\n"
     ]
    }
   ],
   "source": [
    "data_file= '/Users/admin/Documents/Anuj/Coding/Warehouse/harry_potter_3.txt'\n",
    "\n",
    "fp = open(data_file, 'r')\n",
    "data = fp.read()\n",
    "\n",
    "print type(data)\n",
    "\n",
    "print data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 ['\\n', '\\r', '!', ' ', '\"', \"'\", '&', ')', '(', '*', '-', ',', '.', '1', '0', '3', '2', '5', '4', '7', '6', '9', ';', ':', '?', 'A', 'C', 'B', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'J', 'M', 'L', 'O', 'N', 'Q', 'P', 'S', 'R', 'U', 'T', 'W', 'V', 'Y', 'X', 'Z', '\\\\', '_', 'a', '`', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "#get the character set of data\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print len(chars), chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 626260 characters, 80 unique.\n"
     ]
    }
   ],
   "source": [
    "data_size, char_set_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, char_set_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 26, 29, 53, 56, 55, 58]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'ABCDabcd'\n",
    "map(lambda ch: char_to_ix[ch], input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def c2i(input):\n",
    "    return map(lambda ch: char_to_ix[ch], input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def i2c(input):\n",
    "    return map(lambda ch: ix_to_char[ch], input)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------- \n",
    "\n",
    "## Prepare Training data\n",
    "--------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "raw_data = 'abcdefghijklmonpqrstuvwxyzABCDEGHIJKLMONPQRSTUVWXYZ123456789!@#$%^&*()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_len = len(raw_data)\n",
    "print data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "seq_length = 3 # number of steps to unroll the RNN for\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_examples = (data_len - 1) // seq_length\n",
    "print num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_batches = num_examples // batch_size\n",
    "print num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(num_batches):\n",
    "    print \"i = %d\" %i\n",
    "    idx = i * batch_size * seq_length\n",
    "    print \"idx = %d\" %idx\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        start_index = idx + (j*seq_length)\n",
    "        end_index = idx + (j+1)*seq_length\n",
    "        print \"start_index = %d, end_index = %d\" %(start_index, end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_data.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data for an epoch, with batches of size batch_size.\n",
    "\n",
    "def gen_epoch_data(raw_data, batch_size, seq_length=25):\n",
    "    \n",
    "    data_len = len(raw_data)\n",
    "    \n",
    "    # num_examples is num_of_sequences. each sequence is an example\n",
    "    num_examples = (data_len - 1) // seq_length\n",
    "    \n",
    "    num_batches = num_examples // batch_size \n",
    "    \n",
    "    # intuituion for the abovce 2 lines is - we are to feed sequences. If each seq is of length k and there are N \n",
    "    # sequences in each batch - then a batch basically consists of k*N characters; \n",
    "    # thus number of batches = data_len // (seq_lenth * batch_size)\n",
    "    \n",
    "    # An easier way to see this is : num of data points = data_len // seq_length\n",
    "    # therefore num of batches = num of data points // batch_size\n",
    "    \n",
    "    # to get training data, we iterate over batches/chunks/rows. First we establish from where the batch data starts \n",
    "    # imagine data to be a a st line ------------- of length data_len\n",
    "    # we split it into batches \n",
    "    #    ----------------\n",
    "    #    ----------------\n",
    "    #    ----------------\n",
    "    \n",
    "    # These (^) text split into chunks/rows - num_of_chunks =  data_len // (seq_lenth * batch_size)\n",
    "    # first get which chuck to pick. This is done by idx = i * batch_size * seq_length\n",
    "    \n",
    "    # once u have the right row - each point in batch is a seq,seq, we need to pick num_of_piars = batch_size\n",
    "    # so iterate over batch_size\n",
    "    # starting point in a chunk is jth sequence * length of 1 seq\n",
    "    \n",
    "    epoch_data = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch = []\n",
    "        idx = i * batch_size * seq_length\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            start_index = idx + (j*seq_length)\n",
    "            end_index = idx + (j+1)*seq_length\n",
    "            input_ = raw_data[start_index : end_index]\n",
    "            target_ = raw_data[start_index + 1 : end_index +1 ]\n",
    "            \n",
    "            batch.append([c2i(input_), c2i(target_)])\n",
    "            \n",
    "        epoch_data.append(batch)\n",
    "        \n",
    "    return epoch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = gen_epoch_data(raw_data=data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> 782\n",
      "32\n",
      "[[[33, 53, 72, 72, 77, 3, 41, 67, 74, 74, 57, 72, 3, 53, 68, 58, 3, 74, 62, 57, 3, 41, 72, 61, 71], [53, 72, 72, 77, 3, 41, 67, 74, 74, 57, 72, 3, 53, 68, 58, 3, 74, 62, 57, 3, 41, 72, 61, 71, 67]], [[67, 68, 57, 72, 3, 67, 60, 3, 25, 79, 63, 53, 56, 53, 68, 1, 0, 1, 0, 56, 77, 3, 35, 12, 34], [68, 57, 72, 3, 67, 60, 3, 25, 79, 63, 53, 56, 53, 68, 1, 0, 1, 0, 56, 77, 3, 35, 12, 34, 12]], [[12, 3, 43, 67, 75, 66, 61, 68, 59, 1, 0, 1, 0, 26, 33, 25, 41, 45, 28, 43, 3, 38, 39, 28, 1], [3, 43, 67, 75, 66, 61, 68, 59, 1, 0, 1, 0, 26, 33, 25, 41, 45, 28, 43, 3, 38, 39, 28, 1, 0]], [[0, 1, 0, 38, 46, 37, 3, 41, 38, 42, 45, 1, 0, 1, 0, 33, 53, 72, 72, 77, 3, 41, 67, 74, 74], [1, 0, 38, 46, 37, 3, 41, 38, 42, 45, 1, 0, 1, 0, 33, 53, 72, 72, 77, 3, 41, 67, 74, 74, 57]], [[57, 72, 3, 75, 53, 71, 3, 53, 3, 62, 61, 59, 62, 66, 77, 3, 73, 68, 73, 71, 73, 53, 66, 3, 56], [72, 3, 75, 53, 71, 3, 53, 3, 62, 61, 59, 62, 66, 77, 3, 73, 68, 73, 71, 73, 53, 66, 3, 56, 67]], [[67, 77, 3, 61, 68, 3, 65, 53, 68, 77, 3, 75, 53, 77, 71, 12, 3, 31, 67, 72, 3, 67, 68, 57, 3], [77, 3, 61, 68, 3, 65, 53, 68, 77, 3, 75, 53, 77, 71, 12, 3, 31, 67, 72, 3, 67, 68, 57, 3, 74]], [[74, 62, 61, 68, 59, 11, 3, 62, 57, 1, 0, 62, 53, 74, 57, 58, 3, 74, 62, 57, 3, 71, 73, 65, 65], [62, 61, 68, 59, 11, 3, 62, 57, 1, 0, 62, 53, 74, 57, 58, 3, 74, 62, 57, 3, 71, 73, 65, 65, 57]], [[57, 72, 3, 62, 67, 66, 61, 58, 53, 77, 71, 3, 65, 67, 72, 57, 3, 74, 62, 53, 68, 3, 53, 68, 77], [72, 3, 62, 67, 66, 61, 58, 53, 77, 71, 3, 65, 67, 72, 57, 3, 74, 62, 53, 68, 3, 53, 68, 77, 3]], [[3, 67, 74, 62, 57, 72, 3, 74, 61, 65, 57, 3, 67, 60, 3, 77, 57, 53, 72, 12, 3, 31, 67, 72, 3], [67, 74, 62, 57, 72, 3, 74, 61, 65, 57, 3, 67, 60, 3, 77, 57, 53, 72, 12, 3, 31, 67, 72, 3, 53]], [[53, 68, 67, 74, 62, 57, 72, 11, 1, 0, 62, 57, 3, 72, 57, 53, 66, 66, 77, 3, 75, 53, 68, 74, 57], [68, 67, 74, 62, 57, 72, 11, 1, 0, 62, 57, 3, 72, 57, 53, 66, 66, 77, 3, 75, 53, 68, 74, 57, 58]], [[58, 3, 74, 67, 3, 58, 67, 3, 62, 61, 71, 3, 62, 67, 65, 57, 75, 67, 72, 63, 3, 56, 73, 74, 3], [3, 74, 67, 3, 58, 67, 3, 62, 61, 71, 3, 62, 67, 65, 57, 75, 67, 72, 63, 3, 56, 73, 74, 3, 75]], [[75, 53, 71, 3, 60, 67, 72, 55, 57, 58, 3, 74, 67, 3, 58, 67, 3, 61, 74, 3, 61, 68, 3, 71, 57], [53, 71, 3, 60, 67, 72, 55, 57, 58, 3, 74, 67, 3, 58, 67, 3, 61, 74, 3, 61, 68, 3, 71, 57, 55]], [[55, 72, 57, 74, 11, 1, 0, 61, 68, 3, 74, 62, 57, 3, 58, 57, 53, 58, 3, 67, 60, 3, 68, 61, 59], [72, 57, 74, 11, 1, 0, 61, 68, 3, 74, 62, 57, 3, 58, 57, 53, 58, 3, 67, 60, 3, 68, 61, 59, 62]], [[62, 74, 12, 3, 25, 68, 58, 3, 62, 57, 3, 53, 66, 71, 67, 3, 62, 53, 70, 70, 57, 68, 57, 58, 3], [74, 12, 3, 25, 68, 58, 3, 62, 57, 3, 53, 66, 71, 67, 3, 62, 53, 70, 70, 57, 68, 57, 58, 3, 74]], [[74, 67, 3, 56, 57, 3, 53, 3, 75, 61, 79, 53, 72, 58, 12, 1, 0, 1, 0, 32, 74, 3, 75, 53, 71], [67, 3, 56, 57, 3, 53, 3, 75, 61, 79, 53, 72, 58, 12, 1, 0, 1, 0, 32, 74, 3, 75, 53, 71, 3]], [[3, 68, 57, 53, 72, 66, 77, 3, 65, 61, 58, 68, 61, 59, 62, 74, 11, 3, 53, 68, 58, 3, 62, 57, 3], [68, 57, 53, 72, 66, 77, 3, 65, 61, 58, 68, 61, 59, 62, 74, 11, 3, 53, 68, 58, 3, 62, 57, 3, 75]], [[75, 53, 71, 3, 66, 77, 61, 68, 59, 3, 67, 68, 3, 62, 61, 71, 3, 71, 74, 67, 65, 53, 55, 62, 3], [53, 71, 3, 66, 77, 61, 68, 59, 3, 67, 68, 3, 62, 61, 71, 3, 71, 74, 67, 65, 53, 55, 62, 3, 61]], [[61, 68, 3, 56, 57, 58, 11, 3, 74, 62, 57, 1, 0, 56, 66, 53, 68, 63, 57, 74, 71, 3, 58, 72, 53], [68, 3, 56, 57, 58, 11, 3, 74, 62, 57, 1, 0, 56, 66, 53, 68, 63, 57, 74, 71, 3, 58, 72, 53, 75]], [[75, 68, 3, 72, 61, 59, 62, 74, 3, 67, 76, 57, 72, 3, 62, 61, 71, 3, 62, 57, 53, 58, 3, 66, 61], [68, 3, 72, 61, 59, 62, 74, 3, 67, 76, 57, 72, 3, 62, 61, 71, 3, 62, 57, 53, 58, 3, 66, 61, 63]], [[63, 57, 3, 53, 3, 74, 57, 68, 74, 11, 3, 53, 3, 60, 66, 53, 71, 62, 66, 61, 59, 62, 74, 3, 61], [57, 3, 53, 3, 74, 57, 68, 74, 11, 3, 53, 3, 60, 66, 53, 71, 62, 66, 61, 59, 62, 74, 3, 61, 68]], [[68, 3, 67, 68, 57, 3, 62, 53, 68, 58, 1, 0, 53, 68, 58, 3, 53, 3, 66, 53, 72, 59, 57, 3, 66], [3, 67, 68, 57, 3, 62, 53, 68, 58, 1, 0, 53, 68, 58, 3, 53, 3, 66, 53, 72, 59, 57, 3, 66, 57]], [[57, 53, 74, 62, 57, 72, 10, 56, 67, 73, 68, 58, 3, 56, 67, 67, 63, 3, 8, 25, 3, 33, 61, 71, 74], [53, 74, 62, 57, 72, 10, 56, 67, 73, 68, 58, 3, 56, 67, 67, 63, 3, 8, 25, 3, 33, 61, 71, 74, 67]], [[67, 72, 77, 3, 67, 60, 3, 36, 53, 59, 61, 55, 3, 56, 77, 3, 27, 53, 74, 62, 61, 66, 58, 53, 3], [72, 77, 3, 67, 60, 3, 36, 53, 59, 61, 55, 3, 56, 77, 3, 27, 53, 74, 62, 61, 66, 58, 53, 3, 27]], [[27, 53, 59, 71, 62, 67, 74, 7, 1, 0, 70, 72, 67, 70, 70, 57, 58, 3, 67, 70, 57, 68, 3, 53, 59], [53, 59, 71, 62, 67, 74, 7, 1, 0, 70, 72, 67, 70, 70, 57, 58, 3, 67, 70, 57, 68, 3, 53, 59, 53]], [[53, 61, 68, 71, 74, 3, 74, 62, 57, 3, 70, 61, 66, 66, 67, 75, 12, 3, 33, 53, 72, 72, 77, 3, 65], [61, 68, 71, 74, 3, 74, 62, 57, 3, 70, 61, 66, 66, 67, 75, 12, 3, 33, 53, 72, 72, 77, 3, 65, 67]], [[67, 76, 57, 58, 3, 74, 62, 57, 3, 74, 61, 70, 3, 67, 60, 3, 62, 61, 71, 1, 0, 57, 53, 59, 66], [76, 57, 58, 3, 74, 62, 57, 3, 74, 61, 70, 3, 67, 60, 3, 62, 61, 71, 1, 0, 57, 53, 59, 66, 57]], [[57, 10, 60, 57, 53, 74, 62, 57, 72, 3, 69, 73, 61, 66, 66, 3, 58, 67, 75, 68, 3, 74, 62, 57, 3], [10, 60, 57, 53, 74, 62, 57, 72, 3, 69, 73, 61, 66, 66, 3, 58, 67, 75, 68, 3, 74, 62, 57, 3, 70]], [[70, 53, 59, 57, 11, 3, 60, 72, 67, 75, 68, 61, 68, 59, 3, 53, 71, 3, 62, 57, 3, 66, 67, 67, 63], [53, 59, 57, 11, 3, 60, 72, 67, 75, 68, 61, 68, 59, 3, 53, 71, 3, 62, 57, 3, 66, 67, 67, 63, 57]], [[57, 58, 3, 60, 67, 72, 3, 71, 67, 65, 57, 74, 62, 61, 68, 59, 1, 0, 74, 62, 53, 74, 3, 75, 67], [58, 3, 60, 67, 72, 3, 71, 67, 65, 57, 74, 62, 61, 68, 59, 1, 0, 74, 62, 53, 74, 3, 75, 67, 73]], [[73, 66, 58, 3, 62, 57, 66, 70, 3, 62, 61, 65, 3, 75, 72, 61, 74, 57, 3, 62, 61, 71, 3, 57, 71], [66, 58, 3, 62, 57, 66, 70, 3, 62, 61, 65, 3, 75, 72, 61, 74, 57, 3, 62, 61, 71, 3, 57, 71, 71]], [[71, 53, 77, 11, 3, 4, 46, 61, 74, 55, 62, 3, 27, 73, 72, 68, 61, 68, 59, 3, 61, 68, 3, 74, 62], [53, 77, 11, 3, 4, 46, 61, 74, 55, 62, 3, 27, 73, 72, 68, 61, 68, 59, 3, 61, 68, 3, 74, 62, 57]], [[57, 3, 31, 67, 73, 72, 74, 57, 57, 68, 74, 62, 1, 0, 26, 57, 68, 74, 73, 72, 77, 3, 46, 53, 71], [3, 31, 67, 73, 72, 74, 57, 57, 68, 74, 62, 1, 0, 26, 57, 68, 74, 73, 72, 77, 3, 46, 53, 71, 3]]]\n"
     ]
    }
   ],
   "source": [
    "print type(check_data), len(check_data)\n",
    "print len(check_data[0])\n",
    "print check_data[0] # this is a list of lists of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 2e-1\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "#num_epochs = 500\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.int32, shape=(seq_length), name=\"x\")\n",
    "y = tf.placeholder(tf.int32, shape=(seq_length), name=\"y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot representation of the input\n",
    "x_oh = tf.one_hot(indices=x, depth=vocab_size)\n",
    "y_oh = tf.one_hot(indices=y, depth=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(25,)\n",
      "(25, 80)\n",
      "(25, 80)\n"
     ]
    }
   ],
   "source": [
    "#check and balances \n",
    "\n",
    "print x.get_shape()\n",
    "print y.get_shape()\n",
    "\n",
    "print x_oh.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input and output as of now is 25 characters, each in 1-hot format of length 80\n",
    "# we will need to feed it 1 character (in 1-hot format) at a time   \n",
    "\n",
    "#We will convert it into desired format in 2 steps - from (25,80) to 25 tensors of\n",
    "#shape (80,) then to 25 tensors of shape (1,80)\n",
    "\n",
    "# Convert the VxN tensor into list of V tensors each of shape = (V,).\n",
    "rnn_inputs = tf.unpack(value=x_oh)\n",
    "rnn_targets = tf.unpack(value=y_oh)\n",
    "\n",
    "#Unpacks the given dimension of a rank=R tensor into rank=(R-1) tensors\n",
    "#i'th tensor in output is the slice value[i, :]. \n",
    "# given a tensor of shape (A, B, C, D);\n",
    "#If axis == 0 (default value) then the i'th tensor in output is the slice value[i, :, :, :] \n",
    "#and return a list of tensors, where each tensor in output will have shape (B, C, D)\n",
    "\n",
    "# for our data, rank of x_oh is 2 and shape = (25,80)\n",
    "#rank is number of dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(80)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_inputs[0].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the weights and biases.\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    \n",
    "    Wxh = tf.Variable(tf.truncated_normal(shape=(hidden_size, vocab_size)), name='Wxh')\n",
    "    Whh = tf.Variable(tf.truncated_normal(shape=(hidden_size, hidden_size)), name='Whh')\n",
    "    Why = tf.Variable(tf.truncated_normal(shape=(vocab_size, hidden_size)), name='Why')    \n",
    "    bh = tf.Variable(tf.zeros(shape=(hidden_size, 1)), name='bh')\n",
    "    by = tf.Variable(tf.zeros(shape=(vocab_size,1)),name='by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rnn_cell(rnn_input, cur_state):\n",
    "    \n",
    "    # expand_dims is required to make the input a 2-D tensor.\n",
    "    # Given a tensor input, this operation inserts a dimension of 1 at the dimension \n",
    "    # index axis of input's shape. The dimension index axis starts at zero;\n",
    "    \n",
    "    with tf.variable_scope('rnn_cell'):\n",
    "        \n",
    "        inp = tf.expand_dims(input=rnn_input, axis=1)\n",
    "\n",
    "        next_state = tf.tanh(tf.matmul(Wxh, inp) + tf.matmul(Whh, cur_state) + bh)\n",
    "        y_hat = tf.matmul(Why, next_state) + by\n",
    "    \n",
    "    return y_hat, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(80), Dimension(1)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "\n",
    "#initialise state\n",
    "state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# Iterate over all the input vectors\n",
    "for rnn_input in rnn_inputs:\n",
    "    y_hat, state = rnn_cell(rnn_input, state)\n",
    "    # Convert y_hat into a 1-D tensor\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    logits.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper method to compute the softmax losses\n",
    "# (It basically compares the outputs to the expected output)\n",
    "for logit, target in zip(logits, rnn_targets):\n",
    "    losses = [tf.nn.softmax_cross_entropy_with_logits(logit, target)] \n",
    "              \n",
    "# Compute the average loss over the batch\n",
    "total_loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, the operation below computes the gradients and does the backprop!\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([hidden_size, 1])\n",
    "sample_state = init_state\n",
    "\n",
    "seed = tf.placeholder(tf.int32, [1], name='seed')\n",
    "rnn_input = tf.one_hot(seed, vocab_size)\n",
    "rnn_input = tf.squeeze(rnn_input)\n",
    "\n",
    "y_hat, sample_state = rnn_cell(rnn_input, sample_state)\n",
    "\n",
    "prob = tf.nn.softmax(tf.squeeze(y_hat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network defined, lets train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Epoch: 0 ---\n",
      "Epoch: 0 Batch: 0\n",
      "Average training loss in batch: 18.5773375893\n",
      "Average matching chars per batch: 0.3\n",
      "Epoch: 0 Batch: 1\n",
      "Average training loss in batch: 18.0642033322\n",
      "Average matching chars per batch: 0.326\n",
      "Epoch: 0 Batch: 2\n",
      "Average training loss in batch: 18.374355007\n",
      "Average matching chars per batch: 0.44652\n",
      "Epoch: 0 Batch: 3\n",
      "Average training loss in batch: 22.2926209545\n",
      "Average matching chars per batch: 0.3489304\n",
      "Epoch: 0 Batch: 4\n",
      "Average training loss in batch: 19.9001151323\n",
      "Average matching chars per batch: 0.426978608\n",
      "Epoch: 0 Batch: 5\n",
      "Average training loss in batch: 20.8047089863\n",
      "Average matching chars per batch: 0.30853957216\n",
      "Epoch: 0 Batch: 6\n",
      "Average training loss in batch: 22.0076814159\n",
      "Average matching chars per batch: 0.526170791443\n",
      "Epoch: 0 Batch: 7\n",
      "Average training loss in batch: 21.8963731694\n",
      "Average matching chars per batch: 0.490523415829\n",
      "Epoch: 0 Batch: 8\n",
      "Average training loss in batch: 20.2105157658\n",
      "Average matching chars per batch: 0.569810468317\n",
      "Epoch: 0 Batch: 9\n",
      "Average training loss in batch: 19.2089969392\n",
      "Average matching chars per batch: 0.451396209366\n",
      "Epoch: 0 Batch: 10\n",
      "Average training loss in batch: 21.594677763\n",
      "Average matching chars per batch: 0.529027924187\n",
      "Epoch: 0 Batch: 11\n",
      "Average training loss in batch: 18.3977049243\n",
      "Average matching chars per batch: 0.450580558484\n",
      "Epoch: 0 Batch: 12\n",
      "Average training loss in batch: 21.0399586105\n",
      "Average matching chars per batch: 0.52901161117\n",
      "Epoch: 0 Batch: 13\n",
      "Average training loss in batch: 18.4336392026\n",
      "Average matching chars per batch: 0.530580232223\n",
      "Epoch: 0 Batch: 14\n",
      "Average training loss in batch: 22.4564374924\n",
      "Average matching chars per batch: 0.410611604644\n",
      "Epoch: 0 Batch: 15\n",
      "Average training loss in batch: 21.852336164\n",
      "Average matching chars per batch: 0.448212232093\n",
      "Epoch: 0 Batch: 16\n",
      "Average training loss in batch: 21.2325670338\n",
      "Average matching chars per batch: 0.288964244642\n",
      "Epoch: 0 Batch: 17\n",
      "Average training loss in batch: 19.9207976753\n",
      "Average matching chars per batch: 0.385779284893\n",
      "Epoch: 0 Batch: 18\n",
      "Average training loss in batch: 22.1165062141\n",
      "Average matching chars per batch: 0.467715585698\n",
      "Epoch: 0 Batch: 19\n",
      "Average training loss in batch: 20.3239466095\n",
      "Average matching chars per batch: 0.369354311714\n",
      "Epoch: 0 Batch: 20\n",
      "Average training loss in batch: 22.1177225685\n",
      "Average matching chars per batch: 0.547387086234\n",
      "Epoch: 0 Batch: 21\n",
      "Average training loss in batch: 19.7898140955\n",
      "Average matching chars per batch: 0.290947741725\n",
      "Epoch: 0 Batch: 22\n",
      "Average training loss in batch: 18.9268621874\n",
      "Average matching chars per batch: 0.505818954834\n",
      "Epoch: 0 Batch: 23\n",
      "Average training loss in batch: 21.7455857424\n",
      "Average matching chars per batch: 0.670116379097\n",
      "Epoch: 0 Batch: 24\n",
      "Average training loss in batch: 21.5292064381\n",
      "Average matching chars per batch: 0.533402327582\n",
      "Epoch: 0 Batch: 25\n",
      "Average training loss in batch: 20.4064377842\n",
      "Average matching chars per batch: 0.570668046552\n",
      "Epoch: 0 Batch: 26\n",
      "Average training loss in batch: 20.1302134648\n",
      "Average matching chars per batch: 0.371413360931\n",
      "Epoch: 0 Batch: 27\n",
      "Average training loss in batch: 20.4170533103\n",
      "Average matching chars per batch: 0.387428267219\n",
      "Epoch: 0 Batch: 28\n",
      "Average training loss in batch: 19.9651127434\n",
      "Average matching chars per batch: 0.367748565344\n",
      "Epoch: 0 Batch: 29\n",
      "Average training loss in batch: 18.8995351807\n",
      "Average matching chars per batch: 0.527354971307\n",
      "Epoch: 0 Batch: 30\n",
      "Average training loss in batch: 18.2042594522\n",
      "Average matching chars per batch: 0.490547099426\n",
      "Epoch: 0 Batch: 31\n",
      "Average training loss in batch: 20.6378549206\n",
      "Average matching chars per batch: 0.449810941989\n",
      "Epoch: 0 Batch: 32\n",
      "Average training loss in batch: 19.7631169895\n",
      "Average matching chars per batch: 0.44899621884\n",
      "Epoch: 0 Batch: 33\n",
      "Average training loss in batch: 18.4530234365\n",
      "Average matching chars per batch: 0.368979924377\n",
      "Epoch: 0 Batch: 34\n",
      "Average training loss in batch: 21.2599841613\n",
      "Average matching chars per batch: 0.307379598488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-30edc6473315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0msstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mprob_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minp_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mixes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/.virtualenvs/GoogleDL/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/.virtualenvs/GoogleDL/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/.virtualenvs/GoogleDL/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/admin/.virtualenvs/GoogleDL/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/.virtualenvs/GoogleDL/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_data = gen_epoch_data(data, batch_size)\n",
    "\n",
    "tlosses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('./Char_Model.log', sess.graph)\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        \n",
    "        print '--- Starting Epoch:', epoch_idx, '---'\n",
    "        epoch_loss = 0\n",
    "        #epoch_state = np.zeros([hidden_size, 1])\n",
    "        equals = 0.0\n",
    "        \n",
    "        for idx, batch in enumerate(epoch_data):\n",
    "            \n",
    "            training_loss = 0\n",
    "            for example_idx, example in enumerate(batch):\n",
    "                \n",
    "                x_i = example[0]\n",
    "                y_i = example[1]\n",
    "                \n",
    "                #loss, tloss, _, logits_, rnn_targets_, epoch_state = \\\n",
    "                #            sess.run([losses, total_loss, train_step, logits, rnn_targets, state], \\\n",
    "                #            feed_dict={x:x_i, y:y_i})\n",
    "                \n",
    "                loss, tloss, _, logits_, rnn_targets_ = \\\n",
    "                            sess.run([losses, total_loss, optimizer, logits, rnn_targets], \\\n",
    "                            feed_dict={x:x_i, y:y_i})\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                _, tloss,  = sess.run([optimizer, total_loss, logits, rnn_targets], \\\n",
    "                            feed_dict={x:x_i, y:y_i})\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                logits_argmax = np.argmax(logits_, axis=1)\n",
    "                rnn_targets_argmax = np.argmax(rnn_targets_, axis=1)\n",
    "                equals += np.sum(logits_argmax == rnn_targets_argmax)\n",
    "                training_loss += tloss\n",
    "                \n",
    "                if (example_idx % 500 == 0):\n",
    "                    inp_seed = np.array([example[0][0]])\n",
    "\n",
    "                    #print '\\n'\n",
    "                    #print '--- SAMPLE BEGIN ---'\n",
    "                    \n",
    "                    logging.debug(\"\\n\")\n",
    "                    logging.debug(\"--- SAMPLE BEGIN ---\")\n",
    "                    \n",
    "                    num_chars = 100\n",
    "                    ixes = []\n",
    "                    sstate = np.zeros([hidden_size, 1])\n",
    "                    for j in range(num_chars):\n",
    "                        prob_r, sstate = sess.run([prob, sample_state], feed_dict={seed:inp_seed, init_state:sstate, x:x_i})\n",
    "                        ix = np.random.choice(vocab_size, p=prob_r.ravel())\n",
    "                        ixes.append(ix)\n",
    "                        inp_seed = np.array([ix])\n",
    "\n",
    "                    #print ''.join(i2c(ixes))\n",
    "                    #print '--- SAMPLE END ---\\n'\n",
    "                    sent = ''.join(i2c(ixes))\n",
    "                    logging.debug(sent)\n",
    "                    logging.debug(\"--- SAMPLE END ---\\n\")\n",
    "                \n",
    "            training_loss /= len(batch)\n",
    "            equals /= len(batch)\n",
    "            print 'Epoch:', epoch_idx, 'Batch:', idx\n",
    "            print 'Average training loss in batch:', training_loss\n",
    "            print 'Average matching chars per batch:', equals\n",
    "            tlosses.append(training_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tlosses = train()\n",
    "plt.plot(tlosses)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "data_file= '/Users/admin/Documents/Anuj/Coding/Warehouse/input.txt'\n",
    "\n",
    "data = open(data_file, 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 2e-1\n",
    "batch_size = 50\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert an array of chars to array of vocab indices\n",
    "def c2i(inp):\n",
    "    return map(lambda c:char_to_ix[c], inp)\n",
    "\n",
    "def i2c(inp):\n",
    "    return map(lambda c:ix_to_char[c], inp)\n",
    "\n",
    "# Generate data for an epoch, with batches of size batch_size.\n",
    "def gen_epoch_data(raw_data, batch_size):\n",
    "    data_len = len(raw_data)\n",
    "    num_examples = (data_len - 1) // seq_length\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    epoch_data = []\n",
    "    for i in range(num_batches):\n",
    "        batch = []\n",
    "        idx = i * batch_size * seq_length\n",
    "        for j in range(batch_size):\n",
    "            inp = raw_data[idx + j*seq_length:idx + (j+1)*seq_length]\n",
    "            target = raw_data[idx + 1+(j*seq_length):idx + 1+((j+1)*seq_length)]\n",
    "\n",
    "            batch.append([c2i(inp), c2i(target)])\n",
    "        epoch_data.append(batch)\n",
    "    return epoch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_data = gen_epoch_data(data, batch_size)\n",
    "init_state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.int32, shape=(seq_length), name=\"x\")\n",
    "y = tf.placeholder(tf.int32, shape=(seq_length), name=\"y\")\n",
    "state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# One Hot representation of the input\n",
    "x_oh = tf.one_hot(indices=x, depth=vocab_size)\n",
    "y_oh = tf.one_hot(indices=y, depth=vocab_size)\n",
    "\n",
    "rnn_inputs = tf.unpack(x_oh)\n",
    "rnn_targets = tf.unpack(y_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the weights and biases.\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    Wxh = tf.get_variable('Wxh', [hidden_size, vocab_size])\n",
    "    Whh = tf.get_variable('Whh', [hidden_size, hidden_size])\n",
    "    Why = tf.get_variable('Why', [vocab_size, hidden_size])\n",
    "    bh = tf.get_variable('bh', [hidden_size, 1])\n",
    "    by = tf.get_variable('by', [vocab_size, 1])\n",
    "\n",
    "# Actual math behind computing the output and the next state of the RNN.\n",
    "def rnn_cell(rnn_input, cur_state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        Wxh = tf.get_variable('Wxh', [hidden_size, vocab_size])\n",
    "        Whh = tf.get_variable('Whh', [hidden_size, hidden_size])\n",
    "        Why = tf.get_variable('Why', [vocab_size, hidden_size])\n",
    "        bh = tf.get_variable('bh', [hidden_size, 1])\n",
    "        by = tf.get_variable('by', [vocab_size, 1])\n",
    "    inp = tf.expand_dims(rnn_input, 1)\n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(Wxh, inp) + tf.matmul(Whh, cur_state) + bh)\n",
    "    y_hat = tf.matmul(Why, next_state) + by\n",
    "    return y_hat, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    y_hat, state = rnn_cell(rnn_input, state)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    logits.append(y_hat)\n",
    "\n",
    "losses = [tf.nn.softmax_cross_entropy_with_logits(logit, target) for logit, target in zip(logits, rnn_targets)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "num_samples = 25\n",
    "sample_state = init_state\n",
    "seed = tf.placeholder(tf.int32, [1], name='seed')\n",
    "rnn_input = tf.one_hot(seed, vocab_size)\n",
    "ixes = []\n",
    "\n",
    "rnn_input = tf.squeeze(rnn_input)\n",
    "y_hat, sample_state = rnn_cell(rnn_input, sample_state)\n",
    "prob = tf.nn.softmax(tf.squeeze(y_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    tlosses = []\n",
    "    \n",
    "    #save the model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if os.path.isfile(\"model.ckpt\"):\n",
    "            saver.restore(sess, \"model.ckpt\")\n",
    "        else:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            print '--- Starting Epoch:', epoch_idx, '---'\n",
    "            epoch_loss = 0\n",
    "            epoch_state = np.zeros([hidden_size, 1])\n",
    "            equals = 0.0\n",
    "            for idx, batch in enumerate(epoch_data):\n",
    "                training_loss = 0\n",
    "                for example_idx, example in enumerate(batch):\n",
    "                    x_i = example[0]\n",
    "                    y_i = example[1]\n",
    "\n",
    "                    loss, tloss, _, logits_, rnn_targets_, epoch_state = \\\n",
    "                        sess.run([losses, total_loss, train_step, logits, \\\n",
    "                            rnn_targets, state], \\\n",
    "                                feed_dict={x:x_i, y:y_i, init_state:epoch_state}\n",
    "                        )\n",
    "\n",
    "                    logits_argmax = np.argmax(logits_, axis=1)\n",
    "                    rnn_targets_argmax = np.argmax(rnn_targets_, axis=1)\n",
    "                    equals += np.sum(logits_argmax == rnn_targets_argmax)\n",
    "\n",
    "                    training_loss += tloss\n",
    "\n",
    "                    if (example_idx % 100 == 0):\n",
    "                        inp_seed = np.array([example[0][0]])\n",
    "\n",
    "                        print '\\n'\n",
    "                        print '--- SAMPLE BEGIN ---'\n",
    "                        num_chars = 100\n",
    "                        ixes = []\n",
    "                        sstate = np.zeros([hidden_size, 1])\n",
    "                        for j in range(num_chars):\n",
    "                            prob_r, sstate = sess.run([prob, sample_state], feed_dict={seed:inp_seed, init_state:sstate, x:x_i})\n",
    "                            ix = np.random.choice(vocab_size, p=prob_r.ravel())\n",
    "                            ixes.append(ix)\n",
    "                            inp_seed = np.array([ix])\n",
    "\n",
    "                        print ''.join(i2c(ixes))\n",
    "                        print '--- SAMPLE END ---'\n",
    "\n",
    "                training_loss /= len(batch)\n",
    "                equals /= len(batch)\n",
    "                print 'Epoch:', epoch_idx, 'Batch:', idx\n",
    "                print 'Average training loss in batch:', training_loss\n",
    "                print 'Average matching chars per batch:', equals\n",
    "                tlosses.append(training_loss)\n",
    "            save_path = saver.save(sess, \"model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "    return tlosses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tlosses = train()\n",
    "plt.plot(tlosses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
