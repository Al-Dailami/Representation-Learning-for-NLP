{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPython Notebook - N-gram Tutorial\n",
    "\n",
    "I've always wondered how chat bots like [Alice](http://alice.pandorabots.com/) work. Now, they are obviously much more complex than this tutorial will delve into, but we can touch on some of the core principles. One of them is this idea of understanding the relationships between words in sentences. How can we get a machine to understand these relationships?\n",
    "\n",
    "Turns out there's the right way, and then there's the easy way. The right way involves delving deep into [semantic networks](https://en.wikipedia.org/wiki/Semantic_network) and ontologies, something I'd touched upon in my climate modelling days, but never mind that; We're doing **The Easy Way**.\n",
    "\n",
    "### The Easy Way\n",
    "\n",
    "Conversely, the easy way to learn the relationships is by throwing lots of data *en masse* at a machine, and letting it build up a model of the relationships (*this sounds suspiciously like Machine Learning*). \n",
    "\n",
    "An even simpler form of that is to track the number of words that are in sequence with one another, and keeping track of the frequency at which this occurs. We're actually starting to describe something that uses [N-grams](https://en.wikipedia.org/wiki/N-gram). An N-gram is a contiguous (*order matters*) sequence of items, which in this case is the words in text.\n",
    "\n",
    "What we want to do is build up a dictionary of N-grams, which are pairs, triplets or more (*the N*) of words that pop up in the training data, with the value being the number of times they showed up. After we have this dictionary, as a naive example we could actually predict sentences by just randomly choosing words within this dictionary and doing a weighted random sample of the connected words that are part of n-grams within the keys.\n",
    "\n",
    "Lets see how far we can get with N-grams without outside resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 626260)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "path = \"./../data/harry_potter_3.txt\"\n",
    "txt = open(path).read().lower()\n",
    "print('corpus length:', len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108805\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = filter(None, words) # Remove empty strings\n",
    "\n",
    "# Print length of list\n",
    "print len(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words\n",
    "\n",
    "    *Note* : One could use a dictionary instead of a set and keeping count of the occurances gives word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7393\n",
      "['raining', 'foul', 'four', 'woods', 'clotted', 'spiders', 'hanging', 'conjuring', 'conjure', 'sevens', 'increase', 'unblocked', 'electricity', 'wizardry', 'formless', 'wholeheartedly', 'sputter', 'lord', 'flicking', 'sinking']\n"
     ]
    }
   ],
   "source": [
    "#import sets\n",
    "\n",
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "\n",
    "print len(gram1)\n",
    "\n",
    "# Instead of printing all the elements in the set, create an iterator and print 20 elements only\n",
    "gram1_iter = iter(gram1)\n",
    "print [gram1_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like a\n",
      "a much\n",
      "much better\n",
      "better summer\n",
      "summer than\n",
      "than the\n",
      "the last\n",
      "last the\n",
      "the end\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in xrange(len(words)-10, len(words)-1):\n",
    "    print words[i], words[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108804\n",
      "53720\n",
      "[('playing', 'as'), ('tonic', 'was'), ('my', 'final'), ('staring', 'along'), ('of', 'terrified'), ('three', 'minutes'), ('yeh', 'ron'), ('droppin', 'me'), ('not', 'listening'), ('it', 'looks'), ('have', 'made'), ('her', 'job'), ('swishing', 'noise'), ('knuckles', 'of'), ('harry', 'his'), ('fist', 'petunia'), ('do', 'beg'), ('want', 'you'), ('he', 'entered'), ('be', 'fit')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in xrange(len(words)-1)]\n",
    "print len(word_pairs)\n",
    "\n",
    "gram2 = set(word_pairs)\n",
    "print len(gram2)\n",
    "\n",
    "# Print 20 elements from gram2\n",
    "gram2_iter = iter(gram2)\n",
    "print [gram2_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency\n",
    "\n",
    "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4991), ('and', 2620), ('to', 2554), ('a', 2111), ('he', 2065), ('of', 2030), ('harry', 2005), ('was', 1649), ('s', 1545), ('his', 1470), ('said', 1436), ('it', 1398), ('you', 1388), ('i', 1304), ('in', 1201), ('had', 888), ('t', 880), ('that', 862), ('at', 801), ('ron', 756)]\n"
     ]
    }
   ],
   "source": [
    "gram1 = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if gram1.has_key(word):\n",
    "        gram1[word] += 1\n",
    "    else:\n",
    "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1.items(), key=lambda (word, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 461), (('in', 'the'), 292), (('said', 'harry'), 250), (('to', 'the'), 249), (('he', 'was'), 234), (('out', 'of'), 230), (('on', 'the'), 227), (('it', 'was'), 226), (('at', 'the'), 219), (('didn', 't'), 200), (('harry', 's'), 180), (('don', 't'), 171), (('said', 'ron'), 170), (('and', 'hermione'), 162), (('it', 's'), 156), (('he', 'had'), 154), (('into', 'the'), 149), (('he', 'said'), 148), (('was', 'a'), 147), (('in', 'a'), 140)]\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in xrange(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if gram2.has_key(key):\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda (_, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `\"of the\"` and `\"to be\"` are the top two most common 2-grams, sounds good. \n",
    "\n",
    "## Next word prediction\n",
    "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boarhound\n"
     ]
    }
   ],
   "source": [
    "start_word = words[len(words)/4]\n",
    "print start_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just went ahead and chose the word that appears $1/4$ of the way into words, random enough.\n",
    "\n",
    "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
    "\n",
    "    *Note* : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
    "             so you need element[0][0] for first word and element [0][1] for second word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: boarhound\n",
      "2-gram sentence: \" boarhound at the door of the door of the door of the door of the door of the door of \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "\n",
    "word = start_word\n",
    "print \"Start word: %s\" % word\n",
    "\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentence(word, 20)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and hermione s a very good bye harry s a very good bye harry s a very good bye harry \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he was a very good bye harry s a very good bye harry s a very good bye harry s \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she was a very good bye harry s a very good bye harry s a very good bye harry s \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when he was a very good bye harry s a very good bye harry s a very good bye harry \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never seen the door of the door of the door of the door of the door of the door of \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i m not to the door of the door of the door of the door of the door of the \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how could see you know what s a very good bye harry s a very good bye harry s a \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentence(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted random choice based on frequency\n",
    "Same thing. Okay, lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "   total = sum(w for c, w in choices)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         return c\n",
    "      upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and peter you will not got down on george looked as fast but what do i he raised his head \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he s eyes with dusty classroom with his enormous wings fell silent tears ron harry knew what kept to practice \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she said neville regularly went haywire just one that moment please ron says i have found themselves outside the thud \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when we ll be having your books it high over by the ruined painting of explosion grunted ernie mcmillan told \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never speak to punish you complete confidence in a little horns had seen him his teachers found it s reach \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i m coming back to it was walking very confused impression that going to sleep even started again you can \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how dare you re helping himself free hand had an you know the most in their table we re very \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentenceRandom(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that's way more interesting!** Those are starting to look like sentences!\n",
    "\n",
    "    *Note* It's pretty interesting to see that for the sentence \" when he believed him from the amiable but mrs hurst s being ill of being the discussion of course of \", we have hurst s, which we can tell came from Hurst's, an artifact of our stripping away all punctuation but keeping the s.\n",
    "\n",
    "Let's try a longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: it\n",
      "2-gram sentence: \" it so s shrivelfig as though he said hippogriffs attack anyone can explain to him but you could have put herrmone s said harry clutched her precious dogs they lose a lot of us dinner what we ll ever seen that filled with you too said professor trelawney looked over \"\n"
     ]
    }
   ],
   "source": [
    "word = 'it'\n",
    "print \"Start word: %s\" % word\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentenceRandom(word, 50)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, lets see what happens when we go to N-grams above 2.\n",
    "## Tri-grams and more\n",
    "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('out', 'of', 'the'), 96), (('ron', 'and', 'hermione'), 92), (('there', 'was', 'a'), 81), (('i', 'don', 't'), 66), (('in', 'front', 'of'), 45), (('the', 'rest', 'of'), 44), (('harry', 'and', 'hermione'), 42), (('he', 'didn', 't'), 41), (('harry', 'ron', 'and'), 40), (('as', 'though', 'he'), 39), (('harry', 'and', 'ron'), 39), (('the', 'end', 'of'), 38), (('rest', 'of', 'the'), 35), (('harry', 'didn', 't'), 32), (('one', 'of', 'the'), 31), (('was', 'going', 'to'), 31), (('seemed', 'to', 'be'), 29), (('crabbe', 'and', 'goyle'), 28), (('fred', 'and', 'george'), 28), (('said', 'professor', 'mcgonagall'), 27)]\n"
     ]
    }
   ],
   "source": [
    "def generateNgram(n=1):\n",
    "    gram = dict()\n",
    "    \n",
    "    # Some helpers to keep us crashing the PC for now\n",
    "    assert n > 0 and n < 100\n",
    "    \n",
    "    # Populate N-gram dictionary\n",
    "    for i in xrange(len(words)-(n-1)):\n",
    "        key = tuple(words[i:i+n])\n",
    "        if gram.has_key(key):\n",
    "            gram[key] += 1\n",
    "        else:\n",
    "            gram[key] = 1\n",
    "\n",
    "    # Turn into a list of (word, count) sorted by count from most to least\n",
    "    gram = sorted(gram.items(), key=lambda (_, count): -count)\n",
    "    return gram\n",
    "\n",
    "trigram = generateNgram(3)\n",
    "# Print top 20 most frequent ngrams\n",
    "print trigram[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Okay, let's see a selection of sentences for N-grams with N = 2 to 10 and a few starting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-gram list... Done\n",
      "  2-gram: \" and something shiny badge to see said madam pomfrey can get up his bedspread ron \"\n",
      "  2-gram: \" he pleased as hermione we doing nothing i did he was that uncle vernon you \"\n",
      "  2-gram: \" she had been thirteen people reckon filch restore her seat right hand and his eyes \"\n",
      "  2-gram: \" when wizards put on weekends that made his stool and pointed nose out of women \"\n",
      "  2-gram: \" john \"\n",
      "  2-gram: \" never hurt a stone slide then lifted there is it s face was in britain \"\n",
      "  2-gram: \" i haven t be back so that the window and adding your acceleration and hermione \"\n",
      "  2-gram: \" how they had been wearing canary yellow eyes said professor snape doesn t you actualy \"\n",
      "\n",
      "Generating 3-gram list... Done\n",
      "  3-gram: \" and felt just can get any chocolate terrible terrible fate terrible thing so harry saw \"\n",
      "  3-gram: \" he said dumbledore to that he said neville was listed as they haven t got \"\n",
      "  3-gram: \" she s something in honeydukes owners once and ron said hagrid told you were in \"\n",
      "  3-gram: \" when they don t dementors away now being bullied neville did he had been making \"\n",
      "  3-gram: \" john \"\n",
      "  3-gram: \" never had a year before but fell a werewolf taking deep rattling breath as he \"\n",
      "  3-gram: \" i regret it they can skin malfoy s shrivelfig skinned the top grade three broomsticks \"\n",
      "  3-gram: \" how dare you might have mercy a thousand he could do it meant to get \"\n",
      "\n",
      "Generating 4-gram list... Done\n",
      "  4-gram: \" and hermione gasped she s old broom straight before snape his book shivered and flexing \"\n",
      "  4-gram: \" he must be better look at him he gave a run for a very round \"\n",
      "  4-gram: \" she opened harry started to take no one out of catching up and hermione with \"\n",
      "  4-gram: \" when professor lupin you in the fat lady sir cadogan spent most of him to \"\n",
      "  4-gram: \" john \"\n",
      "  4-gram: \" never occurred to gryffindor versus slytherin loses to the galaxy in the door of the \"\n",
      "  4-gram: \" i ll look mildly impressed and the map i think you find another hogsmeade on \"\n",
      "  4-gram: \" how does what she said hermione gasped she chuckled trollishly i was still looking very \"\n",
      "\n",
      "Generating 5-gram list... Done\n",
      "  5-gram: \" and drag after christmas with you can usually reserved for ron how d want to \"\n",
      "  5-gram: \" he said percy out of any other than understand dean waving a right over again \"\n",
      "  5-gram: \" she turned into focus slytherin match by exactly said thank you later fred and uncle \"\n",
      "  5-gram: \" when i went next to their shopping there you ve missed scabbers was so it \"\n",
      "  5-gram: \" john \"\n",
      "  5-gram: \" never met hermione said stan shouted what were usually finds out a packet of fortescue \"\n",
      "  5-gram: \" i will be thanking me ron edged around the bones nearly full except your guardian \"\n",
      "  5-gram: \" how did snape he moaned checking his conversation he told macnair need get to block \"\n",
      "\n",
      "Generating 6-gram list... Done\n",
      "  6-gram: \" and yet seeing him off said lupin wouldn t you know the company as he \"\n",
      "  6-gram: \" he called harry was tilting his grip on the inner circle of messrs moony and \"\n",
      "  6-gram: \" she moaned nooo he jumped back to wash his bed next ron s head boy \"\n",
      "  6-gram: \" when they left hand not be very confused the witch reaching inside yourself killed my \"\n",
      "  6-gram: \" john \"\n",
      "  6-gram: \" never spoken to azkaban must be the cloak hagrid i d done enough it was \"\n",
      "  6-gram: \" i forgot to harry and trembling voice a second said lupin hermione s anti dementor \"\n",
      "  6-gram: \" how you harry stepped out he needed to the field made harry lay crookshanks stood \"\n",
      "\n",
      "Generating 7-gram list... Done\n",
      "  7-gram: \" and his hair but he s desk clutching hands fastened over empty castle this time \"\n",
      "  7-gram: \" he said wood spoke in privet drive harry deserves a dementor tom beckoned them any \"\n",
      "  7-gram: \" she had split second then it be a pair of harry as he knew my \"\n",
      "  7-gram: \" when they were there are don t know we thought they going to us forever \"\n",
      "  7-gram: \" john \"\n",
      "  7-gram: \" never lies alone expecto patronum a bucket looking first on it was serving themselves again \"\n",
      "  7-gram: \" i ve got my office on his nightshirt and glimpsed the setting off him they \"\n",
      "  7-gram: \" how to the executioner macnair s face if he said hermione lookin at christmas harry \"\n",
      "\n",
      "Generating 8-gram list... Done\n",
      "  8-gram: \" and forward on certain defenses said hermione followed by me and laid his remedies are \"\n",
      "  8-gram: \" he d have a humpbacked one of its hooves behind him he didn t say \"\n",
      "  8-gram: \" she was confiscated it goes well think blacks been scared of dangerous creatures teacher like \"\n",
      "  8-gram: \" when he was saying the train as much trouble with sirius black had awoken to \"\n",
      "  8-gram: \" john \"\n",
      "  8-gram: \" never in the required book would cost he roared harry ron whispered anxiously into a \"\n",
      "  8-gram: \" i have been a happy birthday present the goal posts harry no said dumbledore his \"\n",
      "  8-gram: \" how little beak expecting him hermione s not to harry and they went blank so \"\n",
      "\n",
      "Generating 9-gram list... Done\n",
      "  9-gram: \" and join errol harry sped down onto a fit of parchment professor i m betting \"\n",
      "  9-gram: \" he went to eat the fat lady sir cadogan did it off the experience though \"\n",
      "  9-gram: \" she allowed to join forces with interest professor lupin shifted there and hermione began it \"\n",
      "  9-gram: \" when harry ravenclaw no it will raise but before harry looked over so foolhardy he \"\n",
      "  9-gram: \" john \"\n",
      "  9-gram: \" never forget about what really excellent you gave a chair hello harry looked slightly so \"\n",
      "  9-gram: \" i really blow up and took gryffindor table they leaned casually over it to hear \"\n",
      "  9-gram: \" how to where harry the gloom harry and hermione could do said snape came in \"\n"
     ]
    }
   ],
   "source": [
    "def getNGramSentenceRandom(gram, word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "for n in xrange(2,10):\n",
    "    # Generate ngram list\n",
    "    print\n",
    "    print \"Generating %d-gram list...\" % n,\n",
    "    ngram = generateNgram(n)\n",
    "    print \"Done\"\n",
    "    \n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print \"  %d-gram: \\\"\" % n,\n",
    "        getNGramSentenceRandom(ngram, word, 15)\n",
    "        print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the sentences getting better and better with larger n-grams, this correlates to the ngram having more foresight into the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 9-gram list... Done\n"
     ]
    }
   ],
   "source": [
    "# Generate 10gram list\n",
    "print\n",
    "print \"Generating %d-gram list...\" % n,\n",
    "gram10 = generateNgram(10)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the 10gram and see what sort of sentence comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9-gram: \" and they obviously not being louder and hermione kept looking good enough to sniff hopefully it before dumbledore had just just drop he did he was the castle his ink self anymore fact that neville s desk his feet if he d done for him for a fresh attempt to me to harry knew the tiny owl of talent on the now smiling i could they set out of the door it happen when you what s head boy i want to seem to buzz excitedly keep a cackling harry saw you are in called stan old socks well the \"\n",
      "  9-gram: \" he was under his head was filled the disposal o you think about it said harry had finally winning the excitement something of his normally ruddy form when he held him to use it waving his nose smashed into diagon alley and mrs weasley made a stick and stared at harry there that no weeping hagrid had to provoke him and began to the crowd and he seemed to the look of the fire still found excuses to fulfill even himself standing up an enormous surprise hermione p p p s invisibility cloak harry put a good enough things in \"\n",
      "  9-gram: \" she said hermione he was binky didn t bring they cut up today we ve messed around at him fudge was white fog starting to their chocolate into what black who could you want ter open the charm had smashed it so i went to put up surveying snape coolly five minutes to lead them oddsbodikins said hermione there you can you and indicated the nurse came hurtling into sinking he didn t prove buckbeak s front garden even invisibility cloak where there ll look told me said ron his face it several long as he was nobody really expect \"\n",
      "  9-gram: \" when their second floor he forced to mess things those eyes but not going to him on his visor to put her left eye angelina johnson gryffindor team changed his eyes trying to the first the reason to find scabbers was still ten galleons to ern said snape s train rattled the knot on the lesson to communicate to the sky was saying i trusted he ran to gryffindor house you know about black pepper imps they watched the trophy room harry slowly dragging ron weakly though he wasn t the cold went up and other student would be removed \"\n",
      "  9-gram: \" john \"\n",
      "  9-gram: \" never heard what with shock ron ripped the leaky cauldron behind we got past katie and out suggestions for students were representatives weren t know we thought i bet you said my fault it came pelting out of fury and forced to another word harry told o iver wood bitterly they d you the story i m dear you want to kill us the crisis well get an ear neville was away from beneath him said you ve used it is this landing where the nearest dementor problem in the minute petunia cooked a would advise you doing harry alone \"\n",
      "  9-gram: \" i was just told me about he couldn t happy why didn t really happened yeah let s copy and a horrible truth sirius black i wouldn t he d better than anything happened harry harry raised his arms around for the darkness harry was a job said harry could say is he be an old bulletin board end of steam issued suddenly attached themselves against the smiles from knowing i as soon as he knew about black in the window harry pulled scabbers what did you like to us all these twelve years had hurried footsteps on the same \"\n",
      "  9-gram: \" how to try to get into it i m very small struggling against the snitch out he called hoarsely he climbed through the cupboard under a narrow low on the fence the gryffindors were very smart owl office at harry looked horribly like sirius black s head very heavy sickly sort of black and hermione seemed to harry s hump opened his usual harry malfoy in a card the daily prophet s alive just said harry tried it was hoping he heard professor dumbledore slowly they did the execution date to harry he said fred the lake but it said \"\n"
     ]
    }
   ],
   "source": [
    "# Try out a bunch of sentences\n",
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"  %d-gram: \\\"\" % n,\n",
    "    getNGramSentenceRandom(ngram, word, 100)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks almost like normal sentences if you squint a little! Well, that was fun. Next up let's see some ways to improve upon this.\n",
    "\n",
    "Instead of just taking the next word every time, we could take the next k words etc.\n",
    "\n",
    "To be continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 50-gram list... Done\n"
     ]
    }
   ],
   "source": [
    "# Generate 10gram list\n",
    "n = 50\n",
    "print\n",
    "print \"Generating %d-gram list...\" % n,\n",
    "gram30 = generateNgram(n)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50-gram: \" and harry as a tent a danger severus i m coming from father but black said harry was the other \"\n",
      "  50-gram: \" he tried to hogwarts house behind him until last night i m pickling some practice for lunch though he leaned \"\n",
      "  50-gram: \" she was sitting room past dealings with buckbeak he really want to the werewolf out where i could hurt it \"\n",
      "  50-gram: \" when harry said you ll give up into their front knees and the prisoners in the one can t have \"\n",
      "  50-gram: \" john \"\n",
      "  50-gram: \" never trust has he was eighteen moony wormtail padfoot peter he did not going to persuade her throat no professor \"\n",
      "  50-gram: \" i lost her said percy and seeker in your own hedwig and the shops were the news of it meanwhile \"\n",
      "  50-gram: \" how nice bludger ducking beneath beds of fur suddenly stood glaring at the day me said harry roared at the \"\n"
     ]
    }
   ],
   "source": [
    "# Try out a bunch of sentences\n",
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"  %d-gram: \\\"\" % n,\n",
    "    getNGramSentenceRandom(ngram, word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
