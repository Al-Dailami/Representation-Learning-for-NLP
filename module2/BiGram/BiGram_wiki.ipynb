{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# BiGram Model\n",
    "\n",
    "----------\n",
    "\n",
    "### Objective is correctness and clarity of concepts and not efficiency\n",
    "\n",
    "#### Author : Anuj\n",
    "\n",
    "#### Uses Wikipedia Dataset\n",
    "\n",
    "#### Built on lines of  &nbsp;&nbsp;&nbsp;&nbsp;   [representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb](http://localhost:8888/notebooks/representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../Utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from readWikiData import get_wikipedia_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the data file - map tokens to Ids, convert data to Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_data(n_vocab_=1000):\n",
    "    sentences, word2idx = get_wikipedia_data(n_vocab=n_vocab_, n_files=10, by_paragraph=True)\n",
    "    training_data = []\n",
    "    vocab_size = len(word2idx)\n",
    "    for sentence in sentences:\n",
    "        for elem1, elem2 in zip(sentence[:-1], sentence[1:]):\n",
    "            training_data.append((elem1, elem2))\n",
    "    \n",
    "    # this destroys the order of words in a wondow but for bigram its harmless\n",
    "    # all we want is - pair of all bigrams\n",
    "    training_data = list(set(training_data))   \n",
    "    \n",
    "    idx2word = {v:k for k, v in word2idx.iteritems()}\n",
    "    return len(word2idx), training_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size, training_data, word2idx, idx2word = get_wiki_data(n_vocab_=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print vocab_size\n",
    "print type(training_data)\n",
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build validation set - randomly choose 100 keys from idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly pick some validation words from data\n",
    "\n",
    "validation_size = 32\n",
    "#validation_set = random.sample(idx2word.keys(), validation_size)\n",
    "validation_set = idx2word.keys()[500:500+validation_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531]\n",
      "['size', 'st', 'typically', 'model', 'eventually', 'head', 'title', 'far', 'religious', 'films', 'latin', 'television', 'china', 'album', 'born', 'soviet', 'class', 'society', 'strong', 'nature', 'food', 'therefore', 'value', 'includes', 'germany', 'half', 'influence', 'market', 'complex', 'culture', 'women', 'father']\n"
     ]
    }
   ],
   "source": [
    "print validation_set\n",
    "print [idx2word[index] for index in validation_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contains list of pairs that have already been selected\n",
    "bucket_list = []\n",
    "\n",
    "def getNextBatch(bi_grams_, batch_size=1000):\n",
    "    \n",
    "    global bucket_list\n",
    "    \n",
    "    # list of possible pairs to pick from\n",
    "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
    "    \n",
    "    # once you exhaust the possible pais, reset\n",
    "    if len(docs_ids_to_select) < batch_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = bi_grams_\n",
    "        \n",
    "    # Initialize two variables \n",
    "    train_X = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # pick a random chunks of pairs \n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    # Iterate threw all the docs \n",
    "    for item in random_docs:\n",
    "        train_X[index] = item[0]\n",
    "        train_label[index] = item[1]  \n",
    "        index += 1\n",
    "        \n",
    "    #flatten list of lists to a single list\n",
    "    train_X = list(itertools.chain(*train_X))\n",
    "    train_label = list(itertools.chain(*train_label))\n",
    "            \n",
    "    return train_X, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, Y = getNextBatch(bi_grams_=training_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    }
   ],
   "source": [
    "#print len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_batches = len(training_data)/batch_size\n",
    "\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='X_var')\n",
    "Y = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='Y_var')\n",
    "valid_X = tf.Variable(validation_set, dtype=tf.int32, name='X_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oh = tf.one_hot(indices=X, depth=vocab_size, name='Converting_Y_to_Y_oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32,)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print X.get_shape()\n",
    "print Y.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_1 = tf.Variable(tf.truncated_normal(\n",
    "    shape=(vocab_size, embedding_dims),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Embeddings_Matrix\") \n",
    "embeded = tf.nn.embedding_lookup(embedding_layer_1, ids=X, name=\"Embedding_LookUp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(128)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax weights, bias\n",
    "W = tf.Variable(tf.truncated_normal(\n",
    "    shape=(embedding_dims, vocab_size),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Softmax_Weights_Matrix\")\n",
    "b = tf.Variable(tf.zeros(shape=(vocab_size,)), name=\"Softmax_Bias_Vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.add(tf.matmul(embeded, W, name=\"WX\"), b, name=\"WX_plus_b\")\n",
    "\n",
    "#logits = tf.add(tf.matmul(embed, softmax_weights, name=\"WX\"), softmax_bias, name=\"WX_plus_b\")\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_oh, name=\"Compute_Loss\")\n",
    "#mean_loss = tf.reduce_mean(loss)\n",
    "mean_loss = tf.reduce_mean(loss, name=\"Compute_mean_loss\")\n",
    "\n",
    "tf.summary.scalar(\"mean_loss\", mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print logits.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5, name=\"Optimizer\").minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute L2 norm for cosine similarity\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer_1), axis=1, keep_dims=True))\n",
    "normalised_embeddings = embedding_layer_1 / norm\n",
    "\n",
    "# get validation set embeddings\n",
    "validation_data_embeddings = tf.nn.embedding_lookup(normalised_embeddings, ids=valid_X, name=\"validation_embeddings_lookup\")\n",
    "\n",
    "# similarity score of validation embeddings w.r.t normalised= dot product between validation_data_embeddings and mornalised embeddings\n",
    "similarity = tf.matmul(validation_data_embeddings, tf.transpose(normalised_embeddings))  # C.A = C x transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 31.308790\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 31.308790\n",
      "\n",
      "\t Nearest to size :  nearest, aegean, n, gifts, ahead, acceleration, andrew, advice,\n",
      "\t Nearest to st :  not, asl, superman, monopoly, inland, close, scripture, publications,\n",
      "\t Nearest to typically :  postseason, track, lawrence, lions, analogy, temples, pounds, pennant,\n",
      "\t Nearest to model :  counting, teach, domination, january, fall, losing, lincoln, note,\n",
      "\t Nearest to eventually :  technological, embroidery, pedro, atari, reactions, advised, impose, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, much, complement, nomenclature, bernoulli, often,\n",
      "\t Nearest to title :  navigation, evident, andrews, may, finals, squares, appeal, 1844,\n",
      "\t Nearest to far :  regional, thereafter, classics, synonymous, everyday, abilities, therapeutic, northwest,\n",
      "\n",
      "For epoch = 0, batch id = 500, batch loss = 20.307142\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 1000, batch loss = 18.072203\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 1000, batch loss = 18.072203\n",
      "\n",
      "\t Nearest to size :  nearest, n, aegean, ahead, gifts, advice, radius, andrew,\n",
      "\t Nearest to st :  not, asl, atp, monopoly, superman, inland, finish, proper,\n",
      "\t Nearest to typically :  postseason, track, lions, temples, analogy, lawrence, pounds, pennant,\n",
      "\t Nearest to model :  counting, teach, domination, january, note, global, consecrated, livestock,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, assault, concord, requirement,\n",
      "\t Nearest to head :  grid, conceived, shed, often, much, nomenclature, complement, bernoulli,\n",
      "\t Nearest to title :  may, navigation, andrews, evident, 130, finals, squares, 1844,\n",
      "\t Nearest to far :  thereafter, classics, regional, synonymous, everyday, northwest, abilities, therapeutic,\n",
      "\n",
      "For epoch = 0, batch id = 1500, batch loss = 8.694886\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 2000, batch loss = 12.871523\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 2000, batch loss = 12.871523\n",
      "\n",
      "\t Nearest to size :  nearest, n, ahead, aegean, advice, gifts, andrew, radius,\n",
      "\t Nearest to st :  not, asl, atp, monopoly, finish, superman, proper, inland,\n",
      "\t Nearest to typically :  postseason, track, lions, temples, analogy, lawrence, pounds, contribute,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, consecrated, note, livestock,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, assault, concord, requirement,\n",
      "\t Nearest to head :  grid, conceived, shed, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  may, navigation, andrews, 130, evident, finals, squares, an,\n",
      "\t Nearest to far :  classics, regional, thereafter, abilities, northwest, everyday, investigations, optimal,\n",
      "\n",
      "For epoch = 0, batch id = 2500, batch loss = 6.404337\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 3000, batch loss = 6.922733\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 3000, batch loss = 6.922733\n",
      "\n",
      "\t Nearest to size :  nearest, n, ahead, aegean, advice, andrew, gifts, donated,\n",
      "\t Nearest to st :  not, asl, atp, presidency, superman, finish, monopoly, proper,\n",
      "\t Nearest to typically :  postseason, track, lions, temples, analogy, lawrence, contribute, pennant,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, livestock,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, assault, requirement, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  may, navigation, 130, evident, andrews, finals, squares, an,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, everyday, investigations, rapidly,\n",
      "\n",
      "For epoch = 0, batch id = 3500, batch loss = 5.661241\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 4000, batch loss = 5.767599\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 4000, batch loss = 5.767599\n",
      "\n",
      "\t Nearest to size :  nearest, n, ahead, aegean, advice, andrew, gifts, donated,\n",
      "\t Nearest to st :  not, asl, atp, presidency, superman, finish, proper, monopoly,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, contribute, pennant,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, livestock,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, assault, requirement, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, andrews, finals, 130, squares, an,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, rapidly, investigations, moor,\n",
      "\n",
      "For epoch = 0, batch id = 4500, batch loss = 3.896708\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 5000, batch loss = 3.364748\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 5000, batch loss = 3.364748\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, advice, andrew, donated, gifts,\n",
      "\t Nearest to st :  not, asl, atp, presidency, superman, proper, finish, monopoly,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, pennant, contribute,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, requirement, assault, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, 130, an,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 5500, batch loss = 3.723040\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 6000, batch loss = 2.268959\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 6000, batch loss = 2.268959\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, presidency, atp, defended, finish, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, pennant, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, most, requirement, assault, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 6500, batch loss = 0.829195\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 7000, batch loss = 1.170599\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 7000, batch loss = 1.170599\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, presidency, atp, defended, finish, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, pennant, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, reactions, requirement, most, assault, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 7500, batch loss = 1.066534\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 8000, batch loss = 1.182497\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 8000, batch loss = 1.182497\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, presidency, finish, atp, defended, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, pennant, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, concord,\n",
      "\t Nearest to head :  grid, shed, conceived, often, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 8500, batch loss = 0.719281\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 9000, batch loss = 0.527985\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 9000, batch loss = 0.527985\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, presidency, finish, atp, defended, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, analogy, lawrence, pennant, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, concord,\n",
      "\t Nearest to head :  grid, shed, often, conceived, much, nomenclature, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, abilities, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 9500, batch loss = 0.186113\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 10000, batch loss = 0.274564\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 10000, batch loss = 0.274564\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, pennant,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, consecrated, yeast,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, moor, abilities, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 10500, batch loss = 0.106877\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 11000, batch loss = 0.679835\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 11000, batch loss = 0.679835\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, superman, proper,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, pennant,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, moor, rapidly, investigations, ca,\n",
      "\n",
      "For epoch = 0, batch id = 11500, batch loss = 0.038672\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 12000, batch loss = 0.294722\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 12000, batch loss = 0.294722\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, 130,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, moor, rapidly, investigations, ca,\n",
      "\n",
      "For epoch = 0, batch id = 12500, batch loss = 1.354243\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 13000, batch loss = 0.334112\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 13000, batch loss = 0.334112\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, advice, andrew, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, embroidery, requirement, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, squares, an, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, moor, 87, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 13500, batch loss = 0.103036\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 14000, batch loss = 0.465220\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 14000, batch loss = 0.465220\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, andrews, an, squares, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 14500, batch loss = 0.128703\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 15000, batch loss = 0.011801\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 15000, batch loss = 0.011801\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, andrews, squares, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 15500, batch loss = 0.014121\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 16000, batch loss = 0.009270\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 16000, batch loss = 0.009270\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, andrews, squares, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 16500, batch loss = 0.015825\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 17000, batch loss = 0.016190\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 17000, batch loss = 0.016190\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 17500, batch loss = 0.016396\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 18000, batch loss = 0.007790\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 18000, batch loss = 0.007790\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 18500, batch loss = 0.009962\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 19000, batch loss = 0.010249\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 19000, batch loss = 0.010249\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 19500, batch loss = 0.010600\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 20000, batch loss = 0.008487\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 20000, batch loss = 0.008487\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, consecrated,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 20500, batch loss = 0.012237\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 21000, batch loss = 0.015393\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 21000, batch loss = 0.015393\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 21500, batch loss = 0.007130\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 22000, batch loss = 0.231020\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 22000, batch loss = 0.231020\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, asl, finish, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, bernoulli,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 22500, batch loss = 0.452553\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 23000, batch loss = 0.007248\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 23000, batch loss = 0.007248\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, aegean, donated, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, track, temples, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 23500, batch loss = 0.009759\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 24000, batch loss = 0.007950\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 24000, batch loss = 0.007950\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, finals, an, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 24500, batch loss = 0.004508\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 25000, batch loss = 0.007253\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 25000, batch loss = 0.007253\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 25500, batch loss = 0.007468\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 26000, batch loss = 0.006245\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 26000, batch loss = 0.006245\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For epoch = 0, batch id = 26500, batch loss = 0.005152\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 27000, batch loss = 0.007021\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 27000, batch loss = 0.007021\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 27500, batch loss = 0.395552\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 28000, batch loss = 0.005436\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 28000, batch loss = 0.005436\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 28500, batch loss = 0.005491\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 29000, batch loss = 0.003973\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 29000, batch loss = 0.003973\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 29500, batch loss = 0.006907\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 30000, batch loss = 0.006273\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 30000, batch loss = 0.006273\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, moor, rapidly, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 30500, batch loss = 0.006156\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 31000, batch loss = 0.003360\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 31000, batch loss = 0.003360\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 31500, batch loss = 0.003841\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 32000, batch loss = 0.003902\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 32000, batch loss = 0.003902\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 32500, batch loss = 0.005389\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 33000, batch loss = 0.007178\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 33000, batch loss = 0.007178\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 33500, batch loss = 0.007726\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 34000, batch loss = 0.004897\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 34000, batch loss = 0.004897\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 34500, batch loss = 0.005634\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 35000, batch loss = 0.223316\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 35000, batch loss = 0.223316\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, conceived, nomenclature, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 35500, batch loss = 0.011230\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 36000, batch loss = 0.003272\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 36000, batch loss = 0.003272\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 36500, batch loss = 0.003505\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 37000, batch loss = 0.003534\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 37000, batch loss = 0.003534\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 37500, batch loss = 0.004960\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 38000, batch loss = 0.002715\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 38000, batch loss = 0.002715\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 38500, batch loss = 0.003742\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 39000, batch loss = 0.003323\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 39000, batch loss = 0.003323\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 39500, batch loss = 0.004643\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 40000, batch loss = 0.003789\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 40000, batch loss = 0.003789\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 40500, batch loss = 0.003783\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 41000, batch loss = 0.002701\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 41000, batch loss = 0.002701\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 41500, batch loss = 0.002721\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 42000, batch loss = 0.003659\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 42000, batch loss = 0.003659\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 42500, batch loss = 0.002238\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 43000, batch loss = 0.003893\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 43000, batch loss = 0.003893\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 43500, batch loss = 0.003507\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 44000, batch loss = 0.002876\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 44000, batch loss = 0.002876\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 44500, batch loss = 0.003638\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 45000, batch loss = 0.002484\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 45000, batch loss = 0.002484\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 45500, batch loss = 0.002257\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 46000, batch loss = 0.003057\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 46000, batch loss = 0.003057\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 46500, batch loss = 0.002502\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 47000, batch loss = 0.002677\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 47000, batch loss = 0.002677\n",
      "\n",
      "\t Nearest to size :  nearest, ahead, n, donated, aegean, andrew, advice, gifts,\n",
      "\t Nearest to st :  not, finish, asl, presidency, atp, defended, proper, superman,\n",
      "\t Nearest to typically :  postseason, temples, track, lions, lawrence, analogy, preservation, transformed,\n",
      "\t Nearest to model :  teach, counting, domination, global, january, note, yeast, weights,\n",
      "\t Nearest to eventually :  technological, pedro, requirement, embroidery, reactions, most, assault, 130,\n",
      "\t Nearest to head :  grid, shed, often, nomenclature, conceived, much, decreases, american,\n",
      "\t Nearest to title :  navigation, may, evident, an, finals, squares, andrews, engineering,\n",
      "\t Nearest to far :  classics, regional, thereafter, northwest, 87, rapidly, moor, investigations,\n",
      "\n",
      "For epoch = 0, batch id = 47500, batch loss = 0.003218\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2bd1973d1fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNextBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_grams_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f6b50491e7ed>\u001b[0m in \u001b[0;36mgetNextBatch\u001b[0;34m(bi_grams_, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# list of possible pairs to pick from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdocs_ids_to_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_grams_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# once you exhaust the possible pais, reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_epochs = 20\n",
    "LOG_DIR = './bigram_wiki_chk_pts'\n",
    "\n",
    "print \"Number of batches = %d\" %num_batches\n",
    "print \"Number of epochs = %d\" %num_of_epochs\n",
    "\n",
    "\n",
    "validation_size = validation_size/4 # Tempararoy \n",
    "\n",
    "# A SIMPLE saver() to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#batch = BatchData(batch_size=32, list_of_token_ids=data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # writer to write graph to tensorboard\n",
    "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    print \"initialised\\n\"\n",
    "\n",
    "    for epoch_id in range(num_of_epochs):\n",
    "\n",
    "        av_batch_loss = 0\n",
    "\n",
    "        for batch_id in range(num_batches):\n",
    "\n",
    "            X_, Y_ = getNextBatch(bi_grams_=training_data, batch_size=batch_size)\n",
    "\n",
    "            feed_dict = {}\n",
    "            feed_dict[X] = X_\n",
    "            feed_dict[Y] = Y_\n",
    "\n",
    "            batch_loss, _, summary = sess.run([mean_loss, optimizer,summary_op], feed_dict=feed_dict)\n",
    "            \n",
    "            #writer.add_summary(batch_loss, epoch) \n",
    "            writer.add_summary(summary, global_step=epoch_id)\n",
    "\n",
    "            av_batch_loss += batch_loss\n",
    "            \n",
    "            if batch_id % 500 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "            \n",
    "            if batch_id % 1000 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "                \n",
    "                #print validation data\n",
    "                sim = similarity.eval() # compute similarity\n",
    "                \n",
    "                #iterate over each validation example\n",
    "                \n",
    "                for i in range(validation_size):\n",
    "                    word = idx2word[validation_set[i]]\n",
    "                    top_k = 8\n",
    "                    # sort indexes and pick top k. we take 1:top_k+1 since 0th top pick will the same word itself\n",
    "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    #nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    log = '\\t Nearest to %s : ' %word\n",
    "                    for k in range(top_k):\n",
    "                        nearest_word = idx2word[nearest[k]]\n",
    "                        log = '%s %s,' %(log, nearest_word)\n",
    "                    print log        \n",
    "\n",
    "        print \"\\nFor epoch = %d, Av loss = %f\" %(epoch_id, av_batch_loss/num_batches)\n",
    "        \n",
    "        #batch.reset()\n",
    "        \n",
    "    save_path = saver.save(sess, LOG_DIR)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
