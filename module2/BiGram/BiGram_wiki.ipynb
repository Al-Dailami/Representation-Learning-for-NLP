{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# BiGram Model\n",
    "\n",
    "----------\n",
    "\n",
    "#### Built on lines of  &nbsp;&nbsp;&nbsp;&nbsp;   [representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb](http://localhost:8888/notebooks/representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb)\n",
    "\n",
    "### Added \n",
    "    1) tensorboard network visualizations   \n",
    "    2) tensorboard loss visualizations\n",
    "    3) similar words to words in validation data\n",
    "\n",
    "#### Author : Anuj\n",
    "\n",
    "#### Uses Wikipedia Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../../Utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readWikiData import get_wikipedia_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the data file - map tokens to Ids, convert data to Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_data(n_vocab_=1000):\n",
    "    sentences, word2idx = get_wikipedia_data(n_vocab=n_vocab_, n_files=10, by_paragraph=True)\n",
    "    training_data = []\n",
    "    vocab_size = len(word2idx)\n",
    "    for sentence in sentences:\n",
    "        for elem1, elem2 in zip(sentence[:-1], sentence[1:]):\n",
    "            training_data.append((elem1, elem2))\n",
    "    \n",
    "    # this destroys the order of words in a wondow but for bigram its harmless\n",
    "    # all we want is - pair of all bigrams\n",
    "    training_data = list(set(training_data))   \n",
    "    \n",
    "    idx2word = {v:k for k, v in word2idx.iteritems()}\n",
    "    return len(word2idx), training_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size, training_data, word2idx, idx2word = get_wiki_data(n_vocab_=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<type 'list'>\n",
      "1659944\n"
     ]
    }
   ],
   "source": [
    "print vocab_size\n",
    "print type(training_data)\n",
    "print len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build validation set - randomly choose 100 keys from idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick some validation words from data\n",
    "\n",
    "validation_size = 32\n",
    "#validation_set = random.sample(idx2word.keys(), validation_size)\n",
    "validation_set = random.sample(idx2word.keys(), validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6902, 2827, 2229, 6625, 6794, 621, 9610, 4596, 8618, 4533, 2971, 9317, 1997, 2985, 4242, 3519, 4594, 1603, 4897, 8828, 5715, 3371, 3585, 3677, 9640, 1584, 9286, 716, 9339, 7133, 9414, 259]\n",
      "['cpr', 'lay', 'violence', 'charlotte', 'riding', 'technology', '1833', '1932', 'depict', 'camps', 'indians', 'kaye', 'binary', 'celebrated', 'neighboring', 'brian', 'insects', 'plans', 'bowie', 'watson', 'fate', 'progressive', 'pc', 'brazilian', 'worms', 'achieved', 'morphological', 'coast', 'walsh', 'tertiary', 'yard', 'left']\n"
     ]
    }
   ],
   "source": [
    "print validation_set\n",
    "print [idx2word[index] for index in validation_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contains list of pairs that have already been selected\n",
    "bucket_list = []\n",
    "\n",
    "def getNextBatch(bi_grams_, batch_size=1000):\n",
    "    \n",
    "    global bucket_list\n",
    "    \n",
    "    # list of possible pairs to pick from\n",
    "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
    "    \n",
    "    # once you exhaust the possible pais, reset\n",
    "    if len(docs_ids_to_select) < batch_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = bi_grams_\n",
    "        \n",
    "    # Initialize two variables \n",
    "    train_X = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # pick a random chunks of pairs \n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    # Iterate threw all the docs \n",
    "    for item in random_docs:\n",
    "        train_X[index] = item[0]\n",
    "        train_label[index] = item[1]  \n",
    "        index += 1\n",
    "        \n",
    "    #flatten list of lists to a single list\n",
    "    train_X = list(itertools.chain(*train_X))\n",
    "    train_label = list(itertools.chain(*train_label))\n",
    "            \n",
    "    return train_X, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, Y = getNextBatch(bi_grams_=training_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 51873\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_batches = len(training_data)/batch_size\n",
    "\n",
    "print \"Number of batches = %d\" %num_batches\n",
    "\n",
    "\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='X_var')\n",
    "Y = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='Y_var')\n",
    "valid_X = tf.Variable(validation_set, dtype=tf.int32, name='X_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oh = tf.one_hot(indices=X, depth=vocab_size, name='Converting_Y_to_Y_oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32,)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print X.get_shape()\n",
    "print Y.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer_1 = tf.Variable(tf.truncated_normal(\n",
    "    shape=(vocab_size, embedding_dims),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Embeddings_Matrix\") \n",
    "embeded = tf.nn.embedding_lookup(embedding_layer_1, ids=X, name=\"Embedding_LookUp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(128)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax weights, bias\n",
    "W = tf.Variable(tf.truncated_normal(\n",
    "    shape=(embedding_dims, vocab_size),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Softmax_Weights_Matrix\")\n",
    "b = tf.Variable(tf.zeros(shape=(vocab_size,)), name=\"Softmax_Bias_Vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.add(tf.matmul(embeded, W, name=\"WX\"), b, name=\"WX_plus_b\")\n",
    "\n",
    "#logits = tf.add(tf.matmul(embed, softmax_weights, name=\"WX\"), softmax_bias, name=\"WX_plus_b\")\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_oh, name=\"Compute_Loss\")\n",
    "#mean_loss = tf.reduce_mean(loss)\n",
    "mean_loss = tf.reduce_mean(loss, name=\"Compute_mean_loss\")\n",
    "\n",
    "tf.summary.scalar(\"mean_loss\", mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print logits.get_shape()\n",
    "print y_oh.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5, name=\"Optimizer\").minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute L2 norm for cosine similarity\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer_1), axis=1, keep_dims=True))\n",
    "normalised_embeddings = embedding_layer_1 / norm\n",
    "\n",
    "# get validation set embeddings\n",
    "validation_data_embeddings = tf.nn.embedding_lookup(normalised_embeddings, ids=valid_X, name=\"validation_embeddings_lookup\")\n",
    "\n",
    "# similarity score of validation embeddings w.r.t normalised= dot product between validation_data_embeddings and mornalised embeddings\n",
    "similarity = tf.matmul(validation_data_embeddings, tf.transpose(normalised_embeddings))  # C.A = C x transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 51873\n",
      "Number of epochs = 20\n",
      "initialised\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 35.189407\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 35.189407\n",
      "\n",
      "\t Nearest to cpr :  ride, maria, presence, singular, gordon, carolina, hearing, healthy,\n",
      "\t Nearest to lay :  shot, opposition, scripture, alaska, burma, companys, preventing, involvement,\n",
      "\t Nearest to violence :  carl, panama, 21st, differences, fa, 90, label, dismissed,\n",
      "\t Nearest to charlotte :  gut, scotia, presented, stressed, cornwall, lesser, 1821, comet,\n",
      "\t Nearest to riding :  heights, calculation, camp, thrown, proving, compare, calculus, plural,\n",
      "\t Nearest to technology :  genuine, watson, team, flexibility, operations, huge, pointed, fields,\n",
      "\t Nearest to 1833 :  tram, reynolds, 36, choctaw, coined, similarly, wheels, investors,\n",
      "\t Nearest to 1932 :  methodist, alongside, up, stake, morning, reelected, criticism, whose,\n",
      "\n",
      "For epoch = 0, batch id = 500, batch loss = 16.521290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 20\n",
    "LOG_DIR = './bigram_wiki_chk_pts'\n",
    "\n",
    "print \"Number of batches = %d\" %num_batches\n",
    "print \"Number of epochs = %d\" %num_of_epochs\n",
    "\n",
    "\n",
    "validation_size = validation_size/4 # Tempararoy \n",
    "\n",
    "# A SIMPLE saver() to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#batch = BatchData(batch_size=32, list_of_token_ids=data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # writer to write graph to tensorboard\n",
    "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    print \"initialised\\n\"\n",
    "\n",
    "    for epoch_id in range(num_of_epochs):\n",
    "\n",
    "        av_batch_loss = 0\n",
    "\n",
    "        for batch_id in range(num_batches):\n",
    "\n",
    "            X_, Y_ = getNextBatch(bi_grams_=training_data, batch_size=batch_size)\n",
    "\n",
    "            feed_dict = {}\n",
    "            feed_dict[X] = X_\n",
    "            feed_dict[Y] = Y_\n",
    "\n",
    "            batch_loss, _, summary = sess.run([mean_loss, optimizer,summary_op], feed_dict=feed_dict)\n",
    "            \n",
    "            #writer.add_summary(batch_loss, epoch) \n",
    "            writer.add_summary(summary, global_step=epoch_id)\n",
    "\n",
    "            av_batch_loss += batch_loss\n",
    "            \n",
    "            if batch_id % 500 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "            \n",
    "            if batch_id % 1000 == 0:\n",
    "                print \"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss)\n",
    "                \n",
    "                #print validation data\n",
    "                sim = similarity.eval() # compute similarity\n",
    "                \n",
    "                #iterate over each validation example\n",
    "                \n",
    "                for i in range(validation_size):\n",
    "                    word = idx2word[validation_set[i]]\n",
    "                    top_k = 8\n",
    "                    # sort indexes and pick top k. we take 1:top_k+1 since 0th top pick will the same word itself\n",
    "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    #nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    log = '\\t Nearest to %s : ' %word\n",
    "                    for k in range(top_k):\n",
    "                        nearest_word = idx2word[nearest[k]]\n",
    "                        log = '%s %s,' %(log, nearest_word)\n",
    "                    print log        \n",
    "\n",
    "        print \"\\nFor epoch = %d, Av loss = %f\" %(epoch_id, av_batch_loss/num_batches)\n",
    "        \n",
    "        #batch.reset()\n",
    "        \n",
    "    save_path = saver.save(sess, LOG_DIR)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
