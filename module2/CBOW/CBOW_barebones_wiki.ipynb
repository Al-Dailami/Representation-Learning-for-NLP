{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor board projection \n",
    "- Visualizing loss and network on tensorboard\n",
    "- Comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sys.path.append(\"../Utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from readWikiData import get_wikipedia_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, word2idx = get_wikipedia_data(n_files=10, n_vocab=1000, by_paragraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_data_cbow(sentences, word2idx, window_size=5):\n",
    "    training_data = []\n",
    "    vocab_size = len(word2idx)\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) < window_size * 2 + 1:\n",
    "            continue\n",
    "        for i in range(len(sentence)):\n",
    "            left_context = sentence[max(i-window_size, 0): i]\n",
    "            right_context = sentence[i+1:window_size + i + 1]\n",
    "            centre = sentence[i]\n",
    "            \n",
    "            if len(left_context + right_context) < (2*window_size):\n",
    "                len_left = len(left_context)\n",
    "                len_right = len(right_context)\n",
    "                \n",
    "                if len_left < len_right:\n",
    "                    right_context = sentence[i+1 : window_size + i + 1 + (len_right - len_left)]\n",
    "                else:\n",
    "                    left_context = sentence[max(i-window_size - (len_left - len_right), 0): i]\n",
    "            \n",
    "            temp = left_context + right_context\n",
    "            \n",
    "            if len(temp) < window_size * 2:\n",
    "                print sentence\n",
    "                print left_context\n",
    "                print right_context\n",
    "                print centre\n",
    "                break \n",
    "            \n",
    "            training_data.append((tuple(temp), centre))\n",
    "            \n",
    "            \n",
    "    print training_data[:10]\n",
    "    training_data = list(set(training_data))\n",
    "    idx2word = {v:k for k, v in word2idx.iteritems()}\n",
    "    return len(word2idx), training_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((8, 7, 225, 947, 12, 1000, 1000, 1000, 159, 15), 1000), ((1000, 7, 225, 947, 12, 1000, 1000, 1000, 159, 15), 8), ((1000, 8, 225, 947, 12, 1000, 1000, 1000, 159, 15), 7), ((1000, 8, 7, 947, 12, 1000, 1000, 1000, 159, 15), 225), ((1000, 8, 7, 225, 12, 1000, 1000, 1000, 159, 15), 947), ((1000, 8, 7, 225, 947, 1000, 1000, 1000, 159, 15), 12), ((8, 7, 225, 947, 12, 1000, 1000, 159, 15, 1000), 1000), ((7, 225, 947, 12, 1000, 1000, 159, 15, 1000, 1000), 1000), ((225, 947, 12, 1000, 1000, 159, 15, 1000, 1000, 55), 1000), ((947, 12, 1000, 1000, 1000, 15, 1000, 1000, 55, 16), 159)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size, training_data, word2idx, idx2word = get_wiki_data_cbow(sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11866966"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1000, 160, 529, 1000, 1000, 1000, 4, 37, 225, 339), 1000),\n",
       " ((621, 1000, 24, 31, 1000, 2, 594, 1000, 5, 758), 6),\n",
       " ((1000, 44, 22, 1000, 1000, 718, 1000, 5, 24, 248), 17),\n",
       " ((632, 42, 1000, 8, 1000, 2, 510, 1000, 14, 2), 17),\n",
       " ((1000, 2, 1000, 1000, 1000, 6, 1000, 11, 113, 1000), 1000),\n",
       " ((1000, 5, 2, 1000, 242, 677, 25, 1000, 1000, 98), 3),\n",
       " ((1000, 1000, 61, 2, 1000, 1000, 1000, 2, 1000, 427), 4),\n",
       " ((32, 1000, 1000, 16, 45, 1000, 3, 1000, 4, 1000), 9),\n",
       " ((1000, 1000, 5, 2, 1000, 1000, 1000, 1000, 26, 395), 14),\n",
       " ((5, 43, 1000, 6, 1000, 1000, 1000, 19, 37, 1000), 560)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_list = []\n",
    "\n",
    "def getNextBatchCbow(bi_grams_, window_size=5, batch_size=10000):\n",
    "    global bucket_list\n",
    "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
    "    \n",
    "    if len(docs_ids_to_select) < batch_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = bi_grams_\n",
    "        \n",
    "    # Initialize two variables \n",
    "    train_X = np.ndarray(shape=(batch_size, window_size*2), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # Get a random set of docs \n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    # Iterate threw all the docs \n",
    "    for item in random_docs:\n",
    "        train_X[index] = item[0]\n",
    "        train_label[index] = item[1]  \n",
    "        index += 1\n",
    "            \n",
    "    return train_X, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getNextBatchCbow(training_data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's design the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    shape_sum = float(Mi + Mo) \n",
    "    return np.random.uniform(-np.sqrt(6/shape_sum),np.sqrt(6/shape_sum), [Mi, Mo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size_w = 100\n",
    "vocab_size = len(word2idx)\n",
    "n_neg_samples = 20\n",
    "learning_rate = 10e-5\n",
    "epochs = 1001\n",
    "batch_size=10000\n",
    "mu = 0.99\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders for training \n",
    "train_X = tf.placeholder(tf.int32, shape=[batch_size, None])\n",
    "train_label = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define matrix for doc_embedding and word_embedding \n",
    "W1 = tf.Variable(init_weight(vocab_size, embedding_size_w), name=\"W1\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights for the output unit \n",
    "W2 = tf.Variable(init_weight(vocab_size, embedding_size_w), name=\"W2\", dtype=tf.float32)\n",
    "biases = tf.Variable(tf.zeros(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(10000), Dimension(None)]), TensorShape([Dimension(10000), Dimension(1)]), TensorShape([Dimension(1001), Dimension(100)]), TensorShape([Dimension(1001), Dimension(100)]))\n"
     ]
    }
   ],
   "source": [
    "print(train_X.get_shape(), train_label.get_shape(), W1.get_shape(), W2.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = []\n",
    "\n",
    "# generating a vector of size embedding_size_d\n",
    "embed_w = tf.zeros([1, embedding_size_w], dtype=tf.float32)\n",
    "\n",
    "# add all the word vecs in window_size\n",
    "for j in range(window_size*2):\n",
    "    embed_w += tf.nn.embedding_lookup(W1, train_X[:, j])\n",
    "embed.append(embed_w)\n",
    "\n",
    "embed = tf.concat(1, embed)/(window_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embed.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sampled_softmax_loss(weights=W2, \\\n",
    "                                  biases=biases, \\\n",
    "                                  labels=train_label, \\\n",
    "                                  inputs=embed, \\\n",
    "                                  num_sampled=n_neg_samples, \\\n",
    "                                  num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=mu).minimize(loss)\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.01\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = (\n",
    "    tf.train.MomentumOptimizer(learning_rate, momentum=mu).minimize(loss, global_step=global_step)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch :  0  =  3.44129443169\n",
      "Error at epoch :  100  =  2.15142416954\n",
      "Error at epoch :  200  =  1.91867232323\n",
      "Error at epoch :  300  =  1.88222384453\n",
      "Error at epoch :  400  =  1.92942845821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2d039b7388ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtemp_X\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtemp_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNextBatchCbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_grams_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c6cb818b2ffc>\u001b[0m in \u001b[0;36mgetNextBatchCbow\u001b[0;34m(bi_grams_, window_size, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetNextBatchCbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_grams_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbucket_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdocs_ids_to_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_grams_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_ids_to_select\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(epochs):\n",
    "        epoch_error = 0.0\n",
    "        temp_X , temp_labels = getNextBatchCbow(window_size=5, bi_grams_=training_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        feed_dict = {train_X : temp_X, train_label : temp_labels}\n",
    "        \n",
    "        op, l = sess.run([optimizer, loss], \n",
    "                                    feed_dict=feed_dict)\n",
    "        \n",
    "        epoch_error += l\n",
    "                \n",
    "        if step % 100 == 0:\n",
    "            print \"Error at epoch : \", step, \" = \", epoch_error\n",
    "            \n",
    "    save_path = saver.save(sess, \"../models/model_cbow_model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_embedding = None\n",
    "W2_embedding = None \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"../models/model_cbow_model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    # Normalize word2vec \n",
    "    W1_embedding = W1.eval()\n",
    "    \n",
    "    # Normalize word2vec \n",
    "    W2_embedding = W2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = np.mean([W1_embedding, W2_embedding], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Projection of embeddings using t-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE()\n",
    "Z = model.fit_transform(word2vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1])\n",
    "for i in xrange(len(idx2word)):\n",
    "    try:\n",
    "        plt.annotate(s=idx2word[i].encode(\"utf8\"), xy=(Z[i,0], Z[i,1]))\n",
    "    except:\n",
    "        print \"bad string:\", idx2word[i]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
