{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from load_imdb_data import load_imdb_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_data = load_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_data = shuffle(imdb_data)\n",
    "imdb_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>often tagged as a comedy the man in the white ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s easy to make really general comments abou...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think the movie was one sided i watched it r...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have fond memories of watching this visually...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this episode had potential the basic premise o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  often tagged as a comedy the man in the white ...          1\n",
       "1  it s easy to make really general comments abou...         -1\n",
       "2  i think the movie was one sided i watched it r...         -1\n",
       "3  i have fond memories of watching this visually...          1\n",
       "4  this episode had potential the basic premise o...         -1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processDocs(documents, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    This functions takes in a collection of documents and generates a vocabulary based on the size given in input. \n",
    "    It returns a representation for each document in the list of input documents. \n",
    "    \"\"\"\n",
    "    vocab = {} \n",
    "    doc_id = 0 \n",
    "    doc_ids = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_ids.append(doc_id)                          # Give an ID to each document \n",
    "        doc_id += 1\n",
    "        \n",
    "        for word in nltk.word_tokenize(doc):            # Generate a vocabulary while iterating threw the documents \n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1 \n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "    \n",
    "    # Extract the most frequent words based on the vocabulary size \n",
    "    freq_words_list = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    freq_words_set = set([item[0] for item in freq_words_list])\n",
    "    \n",
    "    # Give an index to each word in vocabulary \n",
    "    word2idx = {}         \n",
    "    index_word = 0\n",
    "    for word in freq_words_set:\n",
    "        word2idx[word] = index_word\n",
    "        index_word += 1\n",
    "    word2idx['UNK'] = index_word\n",
    "    \n",
    "    doc_repr = []                          # Represent each document with representation based on the vocabulary  \n",
    "    for doc in documents:\n",
    "        temp = []\n",
    "        for w in doc:\n",
    "            if w in word2idx:\n",
    "                temp.append(word2idx[w])\n",
    "            else:\n",
    "                temp.append(word2idx['UNK'])\n",
    "        doc_repr.append(temp)\n",
    "        \n",
    "    return documents, doc_ids, word2idx, doc_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docs, doc_ids, word2ids, doc_repr = processDocs(imdb_data['review'], vocab_size=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50000, 88863, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(len(docs), len(doc_ids), len(word2ids), len(doc_repr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def performanceTest(X, y, method='tf-idf'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y, \n",
    "                                                        test_size=0.2,  \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y)\n",
    "    \n",
    "    if method == \"tf-idf\":\n",
    "        vectorizer = TfidfVectorizer(min_df=1)\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_pred = clf.predict(X_train)\n",
    "        test_pred = clf.predict(X_test)\n",
    "        print(\"Train F1-score : \", f1_score(y_train, train_pred))\n",
    "        print(\"Test F1-score : \", f1_score(y_test, test_pred))\n",
    "    else:\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_pred = clf.predict(X_train)\n",
    "        test_pred = clf.predict(X_test)\n",
    "        print(\"Train F1-score : \", f1_score(y_train, train_pred))\n",
    "        print(\"Test F1-score : \", f1_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train F1-score : ', 0.7580727121541091)\n",
      "('Test F1-score : ', 0.65170640486208509)\n"
     ]
    }
   ],
   "source": [
    "performanceTest(imdb_data['review'], imdb_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Architecture - PV-DBOW Distributed Bag of Words version of Paragraph Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_size = len(docs)\n",
    "embedding_size_d = 50\n",
    "vocab_size = len(word2ids)\n",
    "window_size = 5\n",
    "n_neg_samples = 10\n",
    "learning_rate = 10e-4\n",
    "epochs = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders for training \n",
    "train_dX = tf.placeholder(tf.int32, shape=[1])\n",
    "train_label = tf.placeholder(tf.int32, shape=[None, window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define matrix for doc_embedding and word_embedding \n",
    "doc_embedding = tf.Variable(tf.random_uniform([doc_size, embedding_size_d], -1.0, 1.0), name=\"doc_embedding\")\n",
    "word_embedding = tf.Variable(tf.random_uniform([embedding_size_d, vocab_size], -1.0, 1.0), name=\"word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define weights for the output unit \n",
    "weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size_d], \n",
    "                                       stddev=1.0 / math.sqrt(vocab_size)))\n",
    "biases = tf.Variable(tf.zeros(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88863, 50)\n"
     ]
    }
   ],
   "source": [
    "print weights.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the doc2vec from the doc_embedding \n",
    "embed = tf.nn.embedding_lookup(doc_embedding, train_dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sampled_softmax_loss(weights=weights, \\\n",
    "                                  biases=biases, \\\n",
    "                                  labels=train_label, \\\n",
    "                                  inputs=embed, \\\n",
    "                                  num_sampled=n_neg_samples, \\\n",
    "                                  num_classes=vocab_size, \\\n",
    "                                  num_true=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch :  0  =  296624.824863\n",
      "Error at epoch :  10  =  276423.430688\n",
      "Error at epoch :  20  =  270631.134126\n",
      "Error at epoch :  30  =  265601.653899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-34461f65d4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m#print feed_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 op, l = sess.run([optimizer, loss], \n\u001b[0;32m---> 21\u001b[0;31m                                     feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mepoch_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(epochs):\n",
    "        epoch_error = 0.0\n",
    "        for id_, repr_ in zip(doc_ids, doc_repr):\n",
    "            if len(repr_) < window_size + 1:\n",
    "                continue\n",
    "            count = random.randint(1, 5)\n",
    "            for cn in range(count):\n",
    "                sample_window = random.sample(repr_, window_size)\n",
    "\n",
    "                feed_dict = {train_dX : [id_], \\\n",
    "                             train_label : np.array([sample_window]).reshape(1, -1)}\n",
    "                                \n",
    "                #print feed_dict\n",
    "                op, l = sess.run([optimizer, loss], \n",
    "                                    feed_dict=feed_dict)\n",
    "                \n",
    "                epoch_error += l\n",
    "                \n",
    "        if step % 10 == 0:\n",
    "            print \"Error at epoch : \", step, \" = \", epoch_error\n",
    "            \n",
    "    save_path = saver.save(sess, \"./models/model_pvdbow.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_pvdbow = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"./models/model_pvdbow.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    # Normalize word2vec \n",
    "    norm_w = tf.sqrt(tf.reduce_sum(tf.square(word_embedding), 1, keep_dims=True))\n",
    "    normalized_word_embeddings = word_embedding / norm_w\n",
    "    word2vec = normalized_word_embeddings.eval()\n",
    "    \n",
    "    # Normalize doc2vec \n",
    "    norm_d = tf.sqrt(tf.reduce_sum(tf.square(doc_embedding), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embedding / norm_d\n",
    "    \n",
    "    # Find performance \n",
    "    performanceTest(normalized_doc_embeddings.eval(), imdb_data['sentiment'][:1000], method=None)\n",
    "    \n",
    "    doc_pvdbow = normalized_doc_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adding batchsizes for speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bucket_list = []\n",
    "\n",
    "def generate_batch_pvdbow(doc_ids, doc_repr, sample_size=5, batch_size=1000, window_size=10):\n",
    "    global bucket_list\n",
    "\n",
    "    docs_ids_to_select = list(set(doc_ids) - set(bucket_list))\n",
    "    \n",
    "    \n",
    "    if len(docs_ids_to_select) < batch_size//sample_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = doc_ids\n",
    "        \n",
    "    index = 0 \n",
    "    train_dX = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size//sample_size)    # Choose set of random documents \n",
    "\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    for id_ in random_docs:\n",
    "        for j in range(sample_size):                                 # Generating a dataset of sample size \n",
    "            random_index = random.randint(0, len(doc_repr[id_]) - window_size)\n",
    "            sample_window = doc_repr[id_][random_index: random_index + window_size]\n",
    "            train_dX[index] = id_\n",
    "            train_label[index] = sample_window\n",
    "            index += 1\n",
    "    return train_dX, train_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc_size = len(docs)\n",
    "embedding_size_d = 100\n",
    "embedding_size_w = 100\n",
    "vocab_size = len(word2ids)\n",
    "window_size = 10\n",
    "n_neg_samples = 10\n",
    "learning_rate = 10e-5\n",
    "epochs = 10001\n",
    "batch_size=1000\n",
    "mu=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders for training \n",
    "train_dX = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "train_label = tf.placeholder(tf.int32, shape=[batch_size, window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define matrix for doc_embedding and word_embedding \n",
    "doc_embedding = tf.Variable(tf.random_uniform([doc_size, embedding_size_d],\n",
    "                                              -np.sqrt(6/(doc_size + embedding_size_d)), \n",
    "                                              np.sqrt(6/(doc_size + embedding_size_d))), \n",
    "                                              name=\"doc_embedding\")\n",
    "word_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size_w],\n",
    "                                               -np.sqrt(6/(doc_size + embedding_size_d)),\n",
    "                                               np.sqrt(6/(doc_size + embedding_size_d)))\n",
    "                                               ,name=\"word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define weights for the output unit \n",
    "weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size_d], \n",
    "                                       stddev=1.0 / math.sqrt(vocab_size)))\n",
    "biases = tf.Variable(tf.zeros(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88863, 100)\n"
     ]
    }
   ],
   "source": [
    "print weights.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the doc2vec from the doc_embedding \n",
    "embed = tf.nn.embedding_lookup(doc_embedding, train_dX[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sampled_softmax_loss(weights=weights, \\\n",
    "                                  biases=biases, \\\n",
    "                                  labels=train_label, \\\n",
    "                                  inputs=embed, \\\n",
    "                                  num_sampled=n_neg_samples, \\\n",
    "                                  num_classes=vocab_size, \\\n",
    "                                  num_true=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nglobal_step = tf.Variable(0, trainable=False)\\nstarter_learning_rate = 0.01\\nlearning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\\n                                           1000, 0.96, staircase=True)\\n# Passing global_step to minimize() will increment it at each step.\\noptimizer = (\\n    tf.train.MomentumOptimizer(learning_rate, momentum=mu).minimize(loss, global_step=global_step)\\n)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=mu).minimize(loss)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\"\"\"\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.01\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = (\n",
    "    tf.train.MomentumOptimizer(learning_rate, momentum=mu).minimize(loss, global_step=global_step)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch :  0  =  2.56065869331\n",
      "Error at epoch :  1000  =  2.6195230484\n",
      "Error at epoch :  2000  =  2.51393556595\n",
      "Error at epoch :  3000  =  2.56772017479\n",
      "Error at epoch :  4000  =  2.62257122993\n",
      "Error at epoch :  5000  =  2.67990541458\n",
      "Error at epoch :  6000  =  2.68699765205\n",
      "Error at epoch :  7000  =  2.66726851463\n",
      "Error at epoch :  8000  =  2.71722745895\n",
      "Error at epoch :  9000  =  2.82101821899\n",
      "Error at epoch :  10000  =  2.49725747108\n",
      "Model saved in file: ../../models/model_pvdbow_batch_training.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(epochs):\n",
    "        epoch_error = 0.0\n",
    "        temp_dX, temp_labels = generate_batch_pvdbow(doc_ids=doc_ids, doc_repr=doc_repr)\n",
    "        feed_dict = {train_dX : temp_dX,train_label : temp_labels}\n",
    "        op, l = sess.run([optimizer, loss], \n",
    "                                    feed_dict=feed_dict)\n",
    "        \n",
    "        epoch_error += l\n",
    "                \n",
    "        if step % 1000 == 0:\n",
    "            print \"Error at epoch : \", step, \" = \", epoch_error\n",
    "            \n",
    "    save_path = saver.save(sess, \"../../models/model_pvdbow_batch_training.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluation of the representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "('Train F1-score : ', 0.0)\n",
      "('Test F1-score : ', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyamsaxena/.virtualenvironments/theano_VM/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "doc_pvdbow = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"../../models/model_pvdbow_batch_training.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    # Normalize word2vec \n",
    "    #norm_w = tf.sqrt(tf.reduce_sum(tf.square(word_embedding), 1, keep_dims=True))\n",
    "    #normalized_word_embeddings = word_embedding / norm_w\n",
    "    #word2vec = normalized_word_embeddings.eval()\n",
    "    \n",
    "    # Normalize doc2vec \n",
    "    #norm_d = tf.sqrt(tf.reduce_sum(tf.square(doc_embedding), 1, keep_dims=True))\n",
    "    #normalized_doc_embeddings = doc_embedding / norm_d\n",
    "    \n",
    "    # Find performance \n",
    "    doc2vec = doc_pvdbow = doc_embedding.eval()\n",
    "    performanceTest(doc2vec, list(imdb_data['sentiment']), method=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_pvdbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_kernel",
   "language": "python",
   "name": "deep_learning_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
